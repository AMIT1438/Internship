{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12c6d14",
   "metadata": {},
   "source": [
    "                                                    ASSIGNMENT-1\n",
    "\n",
    "                                                    WEB SCRAPING\n",
    "                                                    \n",
    "In all the following questions, you have to use BeautifulSoup to scrape different websites and collect data as per\n",
    "the requirement of the question.\n",
    "\n",
    "Every answer to the question should be in form of a python function which should take URL as the parameter.\n",
    "Use Jupyter Notebooks to program, upload it on your GitHub and send the link of the Jupyter notebook to\n",
    "your SME.\n",
    "\n",
    "\n",
    "Submitted by : Amit Puri Goswami\n",
    "\n",
    "Batch No.      :   1843\n",
    "\n",
    "Project: Internship 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3081cad",
   "metadata": {},
   "source": [
    "# 1) Write a python program to display all the header tags from wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1968e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "# https://www.wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "319cf700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of website for which header tags will be scrapped : \")\n",
    "    page = requests.get(url)\n",
    "    page\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    titles = []\n",
    "    for i in soup.find_all(['h1', 'h2','h3','h4','h5','h6']):\n",
    "        titles.append(i.text)\n",
    "    for x in range(len(titles)):\n",
    "        print(titles[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28d6e91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of website for which header tags will be scrapped : https://en.wikipedia.org/wiki/Main_Page\n",
      "Main Page\n",
      "Welcome to Wikipedia\n",
      "From today's featured article\n",
      "Did you know ...\n",
      "In the news\n",
      "On this day\n",
      "Today's featured picture\n",
      "Other areas of Wikipedia\n",
      "Wikipedia's sister projects\n",
      "Wikipedia languages\n",
      "Navigation menu\n",
      "\n",
      "Personal tools\n",
      "\n",
      "\n",
      "Namespaces\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "Navigation\n",
      "\n",
      "\n",
      "Contribute\n",
      "\n",
      "\n",
      "Tools\n",
      "\n",
      "\n",
      "Print/export\n",
      "\n",
      "\n",
      "In other projects\n",
      "\n",
      "\n",
      "Languages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_header()\n",
    "\n",
    "#https://en.wikipedia.org/wiki/Main_Page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb5467",
   "metadata": {},
   "source": [
    "# 2) Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release)\n",
    "and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "313e352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imdb_title():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of imdb website : \")\n",
    "    page = requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    movies_titles = []\n",
    "    for i in soup.find_all('td',class_=\"titleColumn\"):\n",
    "        movies_titles.append(i.text)\n",
    "    for x in range(0,100):\n",
    "        print(movies_titles[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14d687dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of imdb website : https://www.imdb.com/chart/top/\n",
      "\n",
      "      1.\n",
      "      The Shawshank Redemption\n",
      "(1994)\n",
      "\n",
      "\n",
      "      2.\n",
      "      The Godfather\n",
      "(1972)\n",
      "\n",
      "\n",
      "      3.\n",
      "      The Dark Knight\n",
      "(2008)\n",
      "\n",
      "\n",
      "      4.\n",
      "      The Godfather Part II\n",
      "(1974)\n",
      "\n",
      "\n",
      "      5.\n",
      "      12 Angry Men\n",
      "(1957)\n",
      "\n",
      "\n",
      "      6.\n",
      "      Schindler's List\n",
      "(1993)\n",
      "\n",
      "\n",
      "      7.\n",
      "      The Lord of the Rings: The Return of the King\n",
      "(2003)\n",
      "\n",
      "\n",
      "      8.\n",
      "      Pulp Fiction\n",
      "(1994)\n",
      "\n",
      "\n",
      "      9.\n",
      "      The Lord of the Rings: The Fellowship of the Ring\n",
      "(2001)\n",
      "\n",
      "\n",
      "      10.\n",
      "      Il buono, il brutto, il cattivo\n",
      "(1966)\n",
      "\n",
      "\n",
      "      11.\n",
      "      Forrest Gump\n",
      "(1994)\n",
      "\n",
      "\n",
      "      12.\n",
      "      Fight Club\n",
      "(1999)\n",
      "\n",
      "\n",
      "      13.\n",
      "      Inception\n",
      "(2010)\n",
      "\n",
      "\n",
      "      14.\n",
      "      The Lord of the Rings: The Two Towers\n",
      "(2002)\n",
      "\n",
      "\n",
      "      15.\n",
      "      The Empire Strikes Back\n",
      "(1980)\n",
      "\n",
      "\n",
      "      16.\n",
      "      The Matrix\n",
      "(1999)\n",
      "\n",
      "\n",
      "      17.\n",
      "      Goodfellas\n",
      "(1990)\n",
      "\n",
      "\n",
      "      18.\n",
      "      One Flew Over the Cuckoo's Nest\n",
      "(1975)\n",
      "\n",
      "\n",
      "      19.\n",
      "      Se7en\n",
      "(1995)\n",
      "\n",
      "\n",
      "      20.\n",
      "      Shichinin no samurai\n",
      "(1954)\n",
      "\n",
      "\n",
      "      21.\n",
      "      It's a Wonderful Life\n",
      "(1946)\n",
      "\n",
      "\n",
      "      22.\n",
      "      The Silence of the Lambs\n",
      "(1991)\n",
      "\n",
      "\n",
      "      23.\n",
      "      Cidade de Deus\n",
      "(2002)\n",
      "\n",
      "\n",
      "      24.\n",
      "      Saving Private Ryan\n",
      "(1998)\n",
      "\n",
      "\n",
      "      25.\n",
      "      La vita è bella\n",
      "(1997)\n",
      "\n",
      "\n",
      "      26.\n",
      "      The Green Mile\n",
      "(1999)\n",
      "\n",
      "\n",
      "      27.\n",
      "      Interstellar\n",
      "(2014)\n",
      "\n",
      "\n",
      "      28.\n",
      "      Star Wars\n",
      "(1977)\n",
      "\n",
      "\n",
      "      29.\n",
      "      Terminator 2: Judgment Day\n",
      "(1991)\n",
      "\n",
      "\n",
      "      30.\n",
      "      Back to the Future\n",
      "(1985)\n",
      "\n",
      "\n",
      "      31.\n",
      "      Sen to Chihiro no kamikakushi\n",
      "(2001)\n",
      "\n",
      "\n",
      "      32.\n",
      "      Psycho\n",
      "(1960)\n",
      "\n",
      "\n",
      "      33.\n",
      "      The Pianist\n",
      "(2002)\n",
      "\n",
      "\n",
      "      34.\n",
      "      Léon\n",
      "(1994)\n",
      "\n",
      "\n",
      "      35.\n",
      "      Gisaengchung\n",
      "(2019)\n",
      "\n",
      "\n",
      "      36.\n",
      "      The Lion King\n",
      "(1994)\n",
      "\n",
      "\n",
      "      37.\n",
      "      Gladiator\n",
      "(2000)\n",
      "\n",
      "\n",
      "      38.\n",
      "      American History X\n",
      "(1998)\n",
      "\n",
      "\n",
      "      39.\n",
      "      The Departed\n",
      "(2006)\n",
      "\n",
      "\n",
      "      40.\n",
      "      The Usual Suspects\n",
      "(1995)\n",
      "\n",
      "\n",
      "      41.\n",
      "      The Prestige\n",
      "(2006)\n",
      "\n",
      "\n",
      "      42.\n",
      "      Casablanca\n",
      "(1942)\n",
      "\n",
      "\n",
      "      43.\n",
      "      Whiplash\n",
      "(2014)\n",
      "\n",
      "\n",
      "      44.\n",
      "      The Intouchables\n",
      "(2011)\n",
      "\n",
      "\n",
      "      45.\n",
      "      Seppuku\n",
      "(1962)\n",
      "\n",
      "\n",
      "      46.\n",
      "      Hotaru no haka\n",
      "(1988)\n",
      "\n",
      "\n",
      "      47.\n",
      "      Modern Times\n",
      "(1936)\n",
      "\n",
      "\n",
      "      48.\n",
      "      Once Upon a Time in the West\n",
      "(1968)\n",
      "\n",
      "\n",
      "      49.\n",
      "      Top Gun: Maverick\n",
      "(2022)\n",
      "\n",
      "\n",
      "      50.\n",
      "      Rear Window\n",
      "(1954)\n",
      "\n",
      "\n",
      "      51.\n",
      "      Alien\n",
      "(1979)\n",
      "\n",
      "\n",
      "      52.\n",
      "      City Lights\n",
      "(1931)\n",
      "\n",
      "\n",
      "      53.\n",
      "      Nuovo Cinema Paradiso\n",
      "(1988)\n",
      "\n",
      "\n",
      "      54.\n",
      "      Apocalypse Now\n",
      "(1979)\n",
      "\n",
      "\n",
      "      55.\n",
      "      Memento\n",
      "(2000)\n",
      "\n",
      "\n",
      "      56.\n",
      "      Raiders of the Lost Ark\n",
      "(1981)\n",
      "\n",
      "\n",
      "      57.\n",
      "      Django Unchained\n",
      "(2012)\n",
      "\n",
      "\n",
      "      58.\n",
      "      WALL·E\n",
      "(2008)\n",
      "\n",
      "\n",
      "      59.\n",
      "      The Lives of Others\n",
      "(2006)\n",
      "\n",
      "\n",
      "      60.\n",
      "      Sunset Blvd.\n",
      "(1950)\n",
      "\n",
      "\n",
      "      61.\n",
      "      Paths of Glory\n",
      "(1957)\n",
      "\n",
      "\n",
      "      62.\n",
      "      The Shining\n",
      "(1980)\n",
      "\n",
      "\n",
      "      63.\n",
      "      The Great Dictator\n",
      "(1940)\n",
      "\n",
      "\n",
      "      64.\n",
      "      Witness for the Prosecution\n",
      "(1957)\n",
      "\n",
      "\n",
      "      65.\n",
      "      Avengers: Infinity War\n",
      "(2018)\n",
      "\n",
      "\n",
      "      66.\n",
      "      Aliens\n",
      "(1986)\n",
      "\n",
      "\n",
      "      67.\n",
      "      American Beauty\n",
      "(1999)\n",
      "\n",
      "\n",
      "      68.\n",
      "      Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb\n",
      "(1964)\n",
      "\n",
      "\n",
      "      69.\n",
      "      Spider-Man: Into the Spider-Verse\n",
      "(2018)\n",
      "\n",
      "\n",
      "      70.\n",
      "      The Dark Knight Rises\n",
      "(2012)\n",
      "\n",
      "\n",
      "      71.\n",
      "      Oldeuboi\n",
      "(2003)\n",
      "\n",
      "\n",
      "      72.\n",
      "      Joker\n",
      "(2019)\n",
      "\n",
      "\n",
      "      73.\n",
      "      Amadeus\n",
      "(1984)\n",
      "\n",
      "\n",
      "      74.\n",
      "      Braveheart\n",
      "(1995)\n",
      "\n",
      "\n",
      "      75.\n",
      "      Toy Story\n",
      "(1995)\n",
      "\n",
      "\n",
      "      76.\n",
      "      Coco\n",
      "(2017)\n",
      "\n",
      "\n",
      "      77.\n",
      "      Das Boot\n",
      "(1981)\n",
      "\n",
      "\n",
      "      78.\n",
      "      Inglourious Basterds\n",
      "(2009)\n",
      "\n",
      "\n",
      "      79.\n",
      "      Mononoke-hime\n",
      "(1997)\n",
      "\n",
      "\n",
      "      80.\n",
      "      Avengers: Endgame\n",
      "(2019)\n",
      "\n",
      "\n",
      "      81.\n",
      "      Once Upon a Time in America\n",
      "(1984)\n",
      "\n",
      "\n",
      "      82.\n",
      "      Good Will Hunting\n",
      "(1997)\n",
      "\n",
      "\n",
      "      83.\n",
      "      Kimi no na wa.\n",
      "(2016)\n",
      "\n",
      "\n",
      "      84.\n",
      "      Requiem for a Dream\n",
      "(2000)\n",
      "\n",
      "\n",
      "      85.\n",
      "      Toy Story 3\n",
      "(2010)\n",
      "\n",
      "\n",
      "      86.\n",
      "      Singin' in the Rain\n",
      "(1952)\n",
      "\n",
      "\n",
      "      87.\n",
      "      3 Idiots\n",
      "(2009)\n",
      "\n",
      "\n",
      "      88.\n",
      "      Star Wars: Episode VI - Return of the Jedi\n",
      "(1983)\n",
      "\n",
      "\n",
      "      89.\n",
      "      Tengoku to jigoku\n",
      "(1963)\n",
      "\n",
      "\n",
      "      90.\n",
      "      2001: A Space Odyssey\n",
      "(1968)\n",
      "\n",
      "\n",
      "      91.\n",
      "      Eternal Sunshine of the Spotless Mind\n",
      "(2004)\n",
      "\n",
      "\n",
      "      92.\n",
      "      Reservoir Dogs\n",
      "(1992)\n",
      "\n",
      "\n",
      "      93.\n",
      "      Capharnaüm\n",
      "(2018)\n",
      "\n",
      "\n",
      "      94.\n",
      "      Citizen Kane\n",
      "(1941)\n",
      "\n",
      "\n",
      "      95.\n",
      "      Lawrence of Arabia\n",
      "(1962)\n",
      "\n",
      "\n",
      "      96.\n",
      "      Jagten\n",
      "(2012)\n",
      "\n",
      "\n",
      "      97.\n",
      "      M - Eine Stadt sucht einen Mörder\n",
      "(1931)\n",
      "\n",
      "\n",
      "      98.\n",
      "      North by Northwest\n",
      "(1959)\n",
      "\n",
      "\n",
      "      99.\n",
      "      Vertigo\n",
      "(1958)\n",
      "\n",
      "\n",
      "      100.\n",
      "      Idi i smotri\n",
      "(1985)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_imdb_title()\n",
    "\n",
    "#https://www.imdb.com/chart/top/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754bfec4",
   "metadata": {},
   "source": [
    "# 3) Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae70e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind_top100():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of imdb website : \")\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    res = soup.find_all('td')\n",
    "    \n",
    "    movies_titles = []\n",
    "    for i in range(1,500,5):\n",
    "        name = res[i].text.strip().split(\".\")[-1].strip().split(\"\\n\")[0]\n",
    "        ratings = res[i+1].text.strip()\n",
    "        year = res[i].text.strip().split(\".\")[-1].strip().split(\"\\n\")[-1][1:-1]\n",
    "        movies_titles.append([name, year, ratings])\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data=movies_titles,columns=[\"Movie Name\", \"Release Year\", \"Ratings\"])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b0fac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of imdb website : https://www.imdb.com/india/top-rated-indian-movies/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rocketry: The Nambi Effect</td>\n",
       "      <td>2022</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anbe Sivam</td>\n",
       "      <td>2003</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Golmaal</td>\n",
       "      <td>1979</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jai Bhim</td>\n",
       "      <td>2021</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nayakan</td>\n",
       "      <td>1987</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Rang De Basanti</td>\n",
       "      <td>2006</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Baasha</td>\n",
       "      <td>1995</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Baahubali 2: The Conclusion</td>\n",
       "      <td>2017</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Masaan</td>\n",
       "      <td>2015</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Kahaani</td>\n",
       "      <td>2012</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Movie Name Release Year Ratings\n",
       "0    Rocketry: The Nambi Effect         2022     8.5\n",
       "1                    Anbe Sivam         2003     8.4\n",
       "2                       Golmaal         1979     8.4\n",
       "3                      Jai Bhim         2021     8.4\n",
       "4                       Nayakan         1987     8.4\n",
       "..                          ...          ...     ...\n",
       "95              Rang De Basanti         2006     8.0\n",
       "96                       Baasha         1995     8.0\n",
       "97  Baahubali 2: The Conclusion         2017     8.0\n",
       "98                       Masaan         2015     8.0\n",
       "99                      Kahaani         2012     8.0\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = get_ind_top100()\n",
    "d\n",
    "\n",
    "#https://www.imdb.com/india/top-rated-indian-movies/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6ee7f",
   "metadata": {},
   "source": [
    "# 4) Write s python program to display list of respected former presidents of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a9e7d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pres_ind():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of president website : \")\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    pres_ind = []\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    for i in soup.find_all('div',class_=\"presidentListing\"):\n",
    "        pres_ind.append(i.text)\n",
    "    for x in range(0,len(pres_ind)):\n",
    "        print(pres_ind[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7795d223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of president website : https://presidentofindia.nic.in/former-presidents.htm\n",
      "\n",
      "Shri Ram Nath Kovind (birth - 1945)\n",
      "Term of Office: 25 July, 2017 to 25 July, 2022 \n",
      "https://ramnathkovind.nic.in\n",
      "\n",
      "\n",
      "Shri Pranab Mukherjee (1935-2020)\n",
      "Term of Office: 25 July, 2012 to 25 July, 2017 \n",
      "http://pranabmukherjee.nic.in\n",
      "\n",
      "\n",
      "Smt Pratibha Devisingh Patil (birth - 1934)\n",
      "Term of Office: 25 July, 2007 to 25 July, 2012 \n",
      "http://pratibhapatil.nic.in\n",
      "\n",
      "\n",
      "DR. A.P.J. Abdul Kalam (1931-2015)\n",
      "Term of Office: 25 July, 2002 to 25 July, 2007 \n",
      "http://abdulkalam.nic.in\n",
      "\n",
      "\n",
      "Shri K. R. Narayanan (1920 - 2005)\n",
      "Term of Office: 25 July, 1997 to 25 July, 2002 \n",
      "\n",
      "\n",
      "Dr Shankar Dayal Sharma (1918-1999)\n",
      "Term of Office: 25 July, 1992 to 25 July, 1997 \n",
      "\n",
      "\n",
      "Shri R Venkataraman (1910-2009)\n",
      "Term of Office: 25 July, 1987 to 25 July, 1992 \n",
      "\n",
      "\n",
      "Giani Zail Singh (1916-1994)\n",
      "Term of Office: 25 July, 1982 to 25 July, 1987 \n",
      "\n",
      "\n",
      "Shri Neelam Sanjiva Reddy (1913-1996)\n",
      "Term of Office: 25 July, 1977 to 25 July, 1982 \n",
      "\n",
      "\n",
      "Dr. Fakhruddin Ali Ahmed (1905-1977)\n",
      "Term of Office: 24 August, 1974 to 11 February, 1977\n",
      "\n",
      "\n",
      "Shri Varahagiri Venkata Giri (1894-1980)\n",
      "Term of Office: 3 May, 1969 to 20 July, 1969 and 24 August, 1969 to 24 August, 1974\n",
      "\n",
      "\n",
      "Dr. Zakir Husain (1897-1969)\n",
      "Term of Office: 13 May, 1967 to 3 May, 1969\n",
      "\n",
      "\n",
      "Dr. Sarvepalli Radhakrishnan (1888-1975)\n",
      "Term of Office: 13 May, 1962 to 13 May, 1967\n",
      "\n",
      "\n",
      "Dr. Rajendra Prasad (1884-1963) \n",
      "Term of Office: 26 January, 1950 to 13 May, 1962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_pres_ind()\n",
    "\n",
    "#https://presidentofindia.nic.in/former-presidents.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb446764",
   "metadata": {},
   "source": [
    "# 5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:   \n",
    "\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.  \n",
    "\n",
    "b) Top 10 ODI Batsmen along with the records of their team and rating.               \n",
    "\n",
    "c) Top 10 ODI bowlers along with the records of their team and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e522d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer a\n",
    "\n",
    "def top10_odi_men():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of ICC website : \")\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    odimen = []\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    for i in soup.find_all('span',class_=\"u-hide-phablet\"):\n",
    "        odimen.append(i.text)\n",
    "    for x in range(0,10):\n",
    "        print(\"One Day International Rank \",x+1,\" team is : \",odimen[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7648106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of ICC website : https://www.icc-cricket.com/rankings/mens/team-rankings/odi\n",
      "One Day International Rank  1  team is :  New Zealand\n",
      "One Day International Rank  2  team is :  England\n",
      "One Day International Rank  3  team is :  India\n",
      "One Day International Rank  4  team is :  Pakistan\n",
      "One Day International Rank  5  team is :  Australia\n",
      "One Day International Rank  6  team is :  South Africa\n",
      "One Day International Rank  7  team is :  Bangladesh\n",
      "One Day International Rank  8  team is :  Sri Lanka\n",
      "One Day International Rank  9  team is :  West Indies\n",
      "One Day International Rank  10  team is :  Afghanistan\n"
     ]
    }
   ],
   "source": [
    "top10_odi_men()\n",
    "\n",
    "#https://www.icc-cricket.com/rankings/mens/team-rankings/odi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "624e590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer b    https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\n",
    "\n",
    "def top10_batsmen():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of ICC website : \")\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    res = soup.find_all('td')\n",
    "\n",
    "    player = []\n",
    "\n",
    "    for i in range(1,len(res),5):\n",
    "        n1 = res[i].text.strip()\n",
    "        n2 = res[i+1].text.strip()\n",
    "        n3 = res[i+2].text.strip()\n",
    "        n4 = res[i+3].text.strip()\n",
    "        player.append([n1, n2, n3, n4])\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        print(player[i])\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data=player,columns=[\"Player Name\", \"Team\", \"Ratings\", \"Record\"])\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81cab71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of ICC website : https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\n",
      "['Babar Azam', 'PAK', '892', '898 v West Indies, 10/06/2022']\n",
      "['Imam-ul-Haq', 'PAK', '815', '815 v West Indies, 12/06/2022']\n",
      "['Rassie van der Dussen', 'SA', '789', '796 v England, 19/07/2022']\n",
      "['Quinton de Kock', 'SA', '784', '813 v Sri Lanka, 10/03/2019']\n",
      "['Virat Kohli', 'IND', '767', '911 v England, 12/07/2018']\n",
      "['Rohit Sharma', 'IND', '763', '885 v Sri Lanka, 06/07/2019']\n",
      "['Ross Taylor', 'NZ', '744', '841 v Bangladesh, 05/06/2019']\n",
      "['David Warner', 'AUS', '737', '880 v Pakistan, 26/01/2017']\n",
      "['Jonny Bairstow', 'ENG', '732', '796 v India, 26/03/2021']\n",
      "['Aaron Finch', 'AUS', '715', '798 v England, 25/06/2019']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player Name</th>\n",
       "      <th>Team</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Record</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>892</td>\n",
       "      <td>898 v West Indies, 10/06/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>815</td>\n",
       "      <td>815 v West Indies, 12/06/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>789</td>\n",
       "      <td>796 v England, 19/07/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>784</td>\n",
       "      <td>813 v Sri Lanka, 10/03/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>767</td>\n",
       "      <td>911 v England, 12/07/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>763</td>\n",
       "      <td>885 v Sri Lanka, 06/07/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ross Taylor</td>\n",
       "      <td>NZ</td>\n",
       "      <td>744</td>\n",
       "      <td>841 v Bangladesh, 05/06/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>737</td>\n",
       "      <td>880 v Pakistan, 26/01/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>732</td>\n",
       "      <td>796 v India, 26/03/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aaron Finch</td>\n",
       "      <td>AUS</td>\n",
       "      <td>715</td>\n",
       "      <td>798 v England, 25/06/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Player Name Team Ratings                         Record\n",
       "0             Babar Azam  PAK     892  898 v West Indies, 10/06/2022\n",
       "1            Imam-ul-Haq  PAK     815  815 v West Indies, 12/06/2022\n",
       "2  Rassie van der Dussen   SA     789      796 v England, 19/07/2022\n",
       "3        Quinton de Kock   SA     784    813 v Sri Lanka, 10/03/2019\n",
       "4            Virat Kohli  IND     767      911 v England, 12/07/2018\n",
       "5           Rohit Sharma  IND     763    885 v Sri Lanka, 06/07/2019\n",
       "6            Ross Taylor   NZ     744   841 v Bangladesh, 05/06/2019\n",
       "7           David Warner  AUS     737     880 v Pakistan, 26/01/2017\n",
       "8         Jonny Bairstow  ENG     732        796 v India, 26/03/2021\n",
       "9            Aaron Finch  AUS     715      798 v England, 25/06/2019"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=top10_batsmen()\n",
    "a.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f8a2912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer c    https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\n",
    "\n",
    "def top10_bowlermen():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of ICC website : \")\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    res = soup.find_all('td')\n",
    "\n",
    "    player = []\n",
    "\n",
    "    for i in range(1,len(res),5):\n",
    "        n1 = res[i].text.strip()\n",
    "        n2 = res[i+1].text.strip()\n",
    "        n3 = res[i+2].text.strip()\n",
    "        n4 = res[i+3].text.strip()\n",
    "        player.append([n1, n2, n3, n4])\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        print(player[i])\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data=player,columns=[\"Player Name\", \"Team\", \"Ratings\", \"Record\"])\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad5dbb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of ICC website : https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\n",
      "['Trent Boult', 'NZ', '697', '770 v West Indies, 22/06/2019']\n",
      "['Jasprit Bumrah', 'IND', '682', '841 v West Indies, 01/11/2018']\n",
      "['Shaheen Afridi', 'PAK', '681', '688 v West Indies, 10/06/2022']\n",
      "['Josh Hazlewood', 'AUS', '679', '733 v England, 26/01/2018']\n",
      "['Mujeeb Ur Rahman', 'AFG', '676', '712 v Ireland, 24/01/2021']\n",
      "['Mehedi Hasan', 'BAN', '672', '725 v Sri Lanka, 25/05/2021']\n",
      "['Matt Henry', 'NZ', '663', '691 v Bangladesh, 26/03/2021']\n",
      "['Mohammad Nabi', 'AFG', '657', '657 v Zimbabwe, 09/06/2022']\n",
      "['Rashid Khan', 'AFG', '651', '806 v Pakistan, 21/09/2018']\n",
      "['Chris Woakes', 'ENG', '640', '711 v Sri Lanka, 04/07/2021']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player Name</th>\n",
       "      <th>Team</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Record</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>697</td>\n",
       "      <td>770 v West Indies, 22/06/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jasprit Bumrah</td>\n",
       "      <td>IND</td>\n",
       "      <td>682</td>\n",
       "      <td>841 v West Indies, 01/11/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>681</td>\n",
       "      <td>688 v West Indies, 10/06/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>679</td>\n",
       "      <td>733 v England, 26/01/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>676</td>\n",
       "      <td>712 v Ireland, 24/01/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mehedi Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>672</td>\n",
       "      <td>725 v Sri Lanka, 25/05/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>663</td>\n",
       "      <td>691 v Bangladesh, 26/03/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mohammad Nabi</td>\n",
       "      <td>AFG</td>\n",
       "      <td>657</td>\n",
       "      <td>657 v Zimbabwe, 09/06/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rashid Khan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>651</td>\n",
       "      <td>806 v Pakistan, 21/09/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chris Woakes</td>\n",
       "      <td>ENG</td>\n",
       "      <td>640</td>\n",
       "      <td>711 v Sri Lanka, 04/07/2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Player Name Team Ratings                         Record\n",
       "0       Trent Boult   NZ     697  770 v West Indies, 22/06/2019\n",
       "1    Jasprit Bumrah  IND     682  841 v West Indies, 01/11/2018\n",
       "2    Shaheen Afridi  PAK     681  688 v West Indies, 10/06/2022\n",
       "3    Josh Hazlewood  AUS     679      733 v England, 26/01/2018\n",
       "4  Mujeeb Ur Rahman  AFG     676      712 v Ireland, 24/01/2021\n",
       "5      Mehedi Hasan  BAN     672    725 v Sri Lanka, 25/05/2021\n",
       "6        Matt Henry   NZ     663   691 v Bangladesh, 26/03/2021\n",
       "7     Mohammad Nabi  AFG     657     657 v Zimbabwe, 09/06/2022\n",
       "8       Rashid Khan  AFG     651     806 v Pakistan, 21/09/2018\n",
       "9      Chris Woakes  ENG     640    711 v Sri Lanka, 04/07/2021"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= top10_bowlermen()\n",
    "a.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f17c86",
   "metadata": {},
   "source": [
    "# 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "    \n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "\n",
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "291f06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer a\n",
    "\n",
    "def top10_odi_women():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of ICC website : \")\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    odiwomen = []\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    for i in soup.find_all('span',class_=\"u-hide-phablet\"):\n",
    "        odiwomen.append(i.text)\n",
    "    for x in range(0,10):\n",
    "        print(\"Women One Day International Rank \",x+1,\" team is : \",odiwomen[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a31df5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of ICC website : https://www.icc-cricket.com/rankings/womens/team-rankings/odi\n",
      "Women One Day International Rank  1  team is :  Australia\n",
      "Women One Day International Rank  2  team is :  England\n",
      "Women One Day International Rank  3  team is :  South Africa\n",
      "Women One Day International Rank  4  team is :  India\n",
      "Women One Day International Rank  5  team is :  New Zealand\n",
      "Women One Day International Rank  6  team is :  West Indies\n",
      "Women One Day International Rank  7  team is :  Bangladesh\n",
      "Women One Day International Rank  8  team is :  Pakistan\n",
      "Women One Day International Rank  9  team is :  Sri Lanka\n",
      "Women One Day International Rank  10  team is :  Ireland\n"
     ]
    }
   ],
   "source": [
    "top10_odi_women()\n",
    "\n",
    "#https://www.icc-cricket.com/rankings/womens/team-rankings/odi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb8d0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer b    https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\n",
    "\n",
    "def top10_batswomen():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of ICC website : \")\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    res = soup.find_all('td')\n",
    "\n",
    "    player = []\n",
    "\n",
    "    for i in range(1,len(res),5):\n",
    "        n1 = res[i].text.strip()\n",
    "        n2 = res[i+1].text.strip()\n",
    "        n3 = res[i+2].text.strip()\n",
    "        n4 = res[i+3].text.strip()\n",
    "        player.append([n1, n2, n3, n4])\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        print(player[i])\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data=player,columns=[\"Player Name\", \"Team\", \"Ratings\", \"Record\"])\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe303ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of ICC website : https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\n",
      "['Alyssa Healy', 'AUS', '785', '785 v England, 03/04/2022']\n",
      "['Beth Mooney', 'AUS', '749', '748 v England, 03/04/2022']\n",
      "['Natalie Sciver', 'ENG', '747', '755 v South Africa, 15/07/2022']\n",
      "['Laura Wolvaardt', 'SA', '732', '741 v Australia, 22/03/2022']\n",
      "['Meg Lanning', 'AUS', '710', '834 v New Zealand, 24/02/2016']\n",
      "['Rachael Haynes', 'AUS', '701', '713 v West Indies, 15/03/2022']\n",
      "['Amy Satterthwaite', 'NZ', '681', '756 v Australia, 02/03/2017']\n",
      "['Tammy Beaumont', 'ENG', '667', '791 v India, 27/06/2021']\n",
      "['Chamari Athapaththu', 'SL', '655', '691 v South Africa, 14/02/2019']\n",
      "['Smriti Mandhana', 'IND', '649', '797 v England, 28/02/2019']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player Name</th>\n",
       "      <th>Team</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Record</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>785</td>\n",
       "      <td>785 v England, 03/04/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "      <td>748 v England, 03/04/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>747</td>\n",
       "      <td>755 v South Africa, 15/07/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>732</td>\n",
       "      <td>741 v Australia, 22/03/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "      <td>834 v New Zealand, 24/02/2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rachael Haynes</td>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "      <td>713 v West Indies, 15/03/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>681</td>\n",
       "      <td>756 v Australia, 02/03/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tammy Beaumont</td>\n",
       "      <td>ENG</td>\n",
       "      <td>667</td>\n",
       "      <td>791 v India, 27/06/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chamari Athapaththu</td>\n",
       "      <td>SL</td>\n",
       "      <td>655</td>\n",
       "      <td>691 v South Africa, 14/02/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>649</td>\n",
       "      <td>797 v England, 28/02/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Player Name Team Ratings                          Record\n",
       "0         Alyssa Healy  AUS     785       785 v England, 03/04/2022\n",
       "1          Beth Mooney  AUS     749       748 v England, 03/04/2022\n",
       "2       Natalie Sciver  ENG     747  755 v South Africa, 15/07/2022\n",
       "3      Laura Wolvaardt   SA     732     741 v Australia, 22/03/2022\n",
       "4          Meg Lanning  AUS     710   834 v New Zealand, 24/02/2016\n",
       "5       Rachael Haynes  AUS     701   713 v West Indies, 15/03/2022\n",
       "6    Amy Satterthwaite   NZ     681     756 v Australia, 02/03/2017\n",
       "7       Tammy Beaumont  ENG     667         791 v India, 27/06/2021\n",
       "8  Chamari Athapaththu   SL     655  691 v South Africa, 14/02/2019\n",
       "9      Smriti Mandhana  IND     649       797 v England, 28/02/2019"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = top10_batswomen()\n",
    "a.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36421d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer c    https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling\n",
    "\n",
    "def top10_bowlerwomen():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of ICC website : \")\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    res = soup.find_all('td')\n",
    "\n",
    "    player = []\n",
    "\n",
    "    for i in range(1,len(res),5):\n",
    "        n1 = res[i].text.strip()\n",
    "        n2 = res[i+1].text.strip()\n",
    "        n3 = res[i+2].text.strip()\n",
    "        n4 = res[i+3].text.strip()\n",
    "        player.append([n1, n2, n3, n4])\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        print(player[i])\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data=player,columns=[\"Player Name\", \"Team\", \"Ratings\", \"Record\"])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c81c2609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of ICC website : https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling\n",
      "['Sophie Ecclestone', 'ENG', '761', '787 v Bangladesh, 27/03/2022']\n",
      "['Jess Jonassen', 'AUS', '725', '808 v New Zealand, 10/04/2021']\n",
      "['Megan Schutt', 'AUS', '722', '766 v West Indies, 11/09/2019']\n",
      "['Shabnim Ismail', 'SA', '722', '755 v Ireland, 17/06/2022']\n",
      "['Jhulan Goswami', 'IND', '644', '796 v England, 28/02/2007']\n",
      "['Ayabonga Khaka', 'SA', '634', '680 v New Zealand, 17/03/2022']\n",
      "['Rajeshwari Gayakwad', 'IND', '613', '613 v Sri Lanka, 07/07/2022']\n",
      "['Hayley Matthews', 'WI', '612', '644 v Bangladesh, 18/03/2022']\n",
      "['Katherine Brunt', 'ENG', '601', '796 v India, 03/02/2013']\n",
      "['Marizanne Kapp', 'SA', '598', '749 v West Indies, 10/09/2021']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player Name</th>\n",
       "      <th>Team</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Record</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sophie Ecclestone</td>\n",
       "      <td>ENG</td>\n",
       "      <td>761</td>\n",
       "      <td>787 v Bangladesh, 27/03/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>725</td>\n",
       "      <td>808 v New Zealand, 10/04/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Megan Schutt</td>\n",
       "      <td>AUS</td>\n",
       "      <td>722</td>\n",
       "      <td>766 v West Indies, 11/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shabnim Ismail</td>\n",
       "      <td>SA</td>\n",
       "      <td>722</td>\n",
       "      <td>755 v Ireland, 17/06/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jhulan Goswami</td>\n",
       "      <td>IND</td>\n",
       "      <td>644</td>\n",
       "      <td>796 v England, 28/02/2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ayabonga Khaka</td>\n",
       "      <td>SA</td>\n",
       "      <td>634</td>\n",
       "      <td>680 v New Zealand, 17/03/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rajeshwari Gayakwad</td>\n",
       "      <td>IND</td>\n",
       "      <td>613</td>\n",
       "      <td>613 v Sri Lanka, 07/07/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>612</td>\n",
       "      <td>644 v Bangladesh, 18/03/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Katherine Brunt</td>\n",
       "      <td>ENG</td>\n",
       "      <td>601</td>\n",
       "      <td>796 v India, 03/02/2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>598</td>\n",
       "      <td>749 v West Indies, 10/09/2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Player Name Team Ratings                         Record\n",
       "0    Sophie Ecclestone  ENG     761   787 v Bangladesh, 27/03/2022\n",
       "1        Jess Jonassen  AUS     725  808 v New Zealand, 10/04/2021\n",
       "2         Megan Schutt  AUS     722  766 v West Indies, 11/09/2019\n",
       "3       Shabnim Ismail   SA     722      755 v Ireland, 17/06/2022\n",
       "4       Jhulan Goswami  IND     644      796 v England, 28/02/2007\n",
       "5       Ayabonga Khaka   SA     634  680 v New Zealand, 17/03/2022\n",
       "6  Rajeshwari Gayakwad  IND     613    613 v Sri Lanka, 07/07/2022\n",
       "7      Hayley Matthews   WI     612   644 v Bangladesh, 18/03/2022\n",
       "8      Katherine Brunt  ENG     601        796 v India, 03/02/2013\n",
       "9       Marizanne Kapp   SA     598  749 v West Indies, 10/09/2021"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= top10_bowlerwomen()\n",
    "a.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12545ad8",
   "metadata": {},
   "source": [
    "# 7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "        \n",
    "i) Headline\n",
    "\n",
    "ii) Time\n",
    "\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e20fe519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer i    https://www.cnbc.com/world/?region=world\n",
    "\n",
    "def cnbc_headline():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of cnbc website : \")\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    headlines = soup.find_all('li',class_=\"LatestNews-item\")\n",
    "    for x in headlines:\n",
    "        print(x.text.strip(\"Ago\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7632b739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of cnbc website : https://www.cnbc.com/world/?region=world\n",
      "11 Min AgoBiden says he's 'not worried' about China's increased aggression toward Taiwan\n",
      "18 Min AgoConsumer confidence in housing just hit the lowest point in over a decade\n",
      "19 Min AgoWhat the Inflation Reduction Act means for the buyback 'monsters'\n",
      "26 Min AgoConsumer inflation expectations took a major dive in July,  Fed survey shows\n",
      "31 Min AgoAxios to sell itself to Cox Enterprises for a reported $525 million\n",
      "37 Min AgoGoldman says beware these stocks in a tough environment ahead\n",
      "1 Hour AgoFinancial psychologist: Beware of money advice on social media\n",
      "2 Hours AgoSarat Sethi has some stock picks to play inflation and a consumer slowdown\n",
      "2 Hours AgoFor tech workers who missed IPO payday, time for job market rethink\n",
      "2 Hours AgoNvidia warns on second-quarter revenue, shares dip\n",
      "2 Hours AgoBed Bath & Beyond shares jump more than 60% as message board mentions soar \n",
      "2 Hours AgoSoftBank dumps its entire stake in Uber as losses mount at its investment unit\n",
      "2 Hours AgoWhat Cramer is watching Monday — IRA Act lifts EV and solar stocks, Pfizer's buy\n",
      "2 Hours AgoAn explanation of the 'pain trade' on Wall Street\n",
      "2 Hours Ago'Echoes of the Cold War' as Blinken heads to Africa, vying with Russia for influence\n",
      "3 Hours AgoMonday's biggest analyst calls: Tesla, Amazon, First Solar, Roku, Apple, Carvana\n",
      "3 Hours AgoGoldman Sachs says buy this biopharma stock that's poised to rally nearly 40%\n",
      "3 Hours AgoDeadlock over Nord Stream gas turbine is not our fault: Siemens Energy CEO\n",
      "4 Hours AgoStocks making the biggest moves in the premarket: Palantir, Signify Health and more\n",
      "4 Hours AgoCash withdrawals in the UK soar as Brits grapple with the rising cost of livin\n",
      "4 Hours AgoPalantir shares fall more than 12% following earnings report \n",
      "4 Hours Ago5 things to know before the stock market opens Monday\n",
      "4 Hours AgoJPMorgan upgrades First Solar, says climate bill could help boost the shares\n",
      "5 Hours AgoRoku downgraded to sell by Pivotal, who says the stock should be shorted\n",
      "5 Hours AgoThe market's biggest winners and losers in the climate, health and tax bill\n",
      "6 Hours AgoJPMorgan downgrades shares of Carvana, cites risks to profitability in a worsening macro environment\n",
      "6 Hours AgoUN chief calls latest Russian attack on nuclear plant 'suicidal'; Zaporizhzhia set for referendum\n",
      "7 Hours AgoU.S. Treasury yields fall as investors weigh Fed rate hike outlook\n",
      "9 Hours AgoSoftBank posts a $21.6 billion quarterly loss on its Vision Fund\n",
      "9 Hours AgoEuropean stocks climb as traders assess earnings, economic data; Stoxx 600 up 1%\n"
     ]
    }
   ],
   "source": [
    "cnbc_headline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d2400432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer ii  https://www.cnbc.com/world/?region=world\n",
    "def cnbc_time():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of cnbc website : \")\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    headlines = soup.find('span',class_=\"MarketCard-lastTime\")\n",
    "    print(headlines)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1365ecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of cnbc website : https://www.cnbc.com/world/?region=world\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnbc_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cbefadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer iii   https://www.cnbc.com/world/?region=world\n",
    "def cnbc_newslink():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of cnbc website : \")\n",
    "    page = requests.get(url)\n",
    "    temp = page.content\n",
    "    \n",
    "    soup = BeautifulSoup(temp, 'html5lib')\n",
    "    news_link = soup.find_all('div', class_='LatestNews-container')\n",
    "    for i in range(0,len(news_link)):\n",
    "        a = news_link[i].find('a')['href']\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a61d5c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of cnbc website : https://www.cnbc.com/world/?region=world\n",
      "/pro/\n",
      "https://www.cnbc.com/2022/08/08/top-gun-maverick-becomes-7th-highest-grossing-film-domestically.html\n",
      "/pro/\n",
      "https://www.cnbc.com/2022/08/08/these-30-companies-will-help-employees-pay-off-their-student-loans.html\n",
      "/investingclub/\n",
      "https://www.cnbc.com/2022/08/08/reconciliation-bill-includes-nearly-80-billion-for-irs-funding.html\n",
      "https://www.cnbc.com/2022/08/08/biden-says-hes-not-worried-about-chinas-increased-aggression-toward-taiwan-following-pelosi-visit.html\n",
      "https://www.cnbc.com/2022/08/08/consumer-confidence-in-housing-market-hits-lowest-in-over-a-decade.html\n",
      "/pro/\n",
      "https://www.cnbc.com/2022/08/08/consumers-expectations-of-future-inflation-decreased-significantly-in-win-for-the-federal-reserve.html\n",
      "https://www.cnbc.com/2022/08/08/axios-to-sell-itself-to-cox-enterprises-for-525-million.html\n",
      "/pro/\n",
      "https://www.cnbc.com/2022/08/08/financial-psychologist-beware-of-money-advice-on-social-media.html\n",
      "/pro/\n",
      "https://www.cnbc.com/2022/08/08/for-tech-workers-who-missed-ipo-payday-time-for-job-market-rethink.html\n",
      "https://www.cnbc.com/2022/08/08/nvidia-warns-on-second-quarter-revenue-shares-dip.html\n",
      "https://www.cnbc.com/2022/08/08/bed-bath-beyond-shares-jump-30percent-as-message-board-mentions-soar.html\n",
      "https://www.cnbc.com/2022/08/08/softbank-sells-entire-stake-in-uber-as-vision-fund-losses-mount.html\n",
      "/investingclub/\n",
      "/pro/\n",
      "https://www.cnbc.com/2022/08/08/echoes-of-the-cold-war-as-blinken-heads-to-africa-vying-with-russia-for-influence.html\n",
      "/pro/\n",
      "/pro/\n",
      "https://www.cnbc.com/2022/08/08/russia-gas-siemens-energy-ceo-says-turbine-dispute-is-not-our-fault.html\n",
      "https://www.cnbc.com/2022/08/08/stocks-making-the-biggest-moves-in-the-premarket-palantir-signify-health-global-blood-therapeutics-and-more.html\n",
      "https://www.cnbc.com/2022/08/08/uk-cash-withdrawals-hit-a-record-high-as-brits-grapple-with-inflation.html\n",
      "https://www.cnbc.com/2022/08/08/palantir-pltr-earnings-q2-2022-.html\n",
      "https://www.cnbc.com/2022/08/08/5-things-to-know-before-the-stock-market-opens-monday-august-8.html\n",
      "/pro/\n",
      "/pro/\n"
     ]
    }
   ],
   "source": [
    "cnbc_newslink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f327c2",
   "metadata": {},
   "source": [
    "# 8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days. \n",
    "\n",
    "https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "\n",
    "Scrape below mentioned details :\n",
    "    \n",
    "i) Paper Title\n",
    "\n",
    "ii) Authors\n",
    "\n",
    "iii) Published Date\n",
    "\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c6f78be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer i   https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "def journal_name():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of cnbc website : \")\n",
    "    page = requests.get(url)\n",
    "    temp = page.content\n",
    "    \n",
    "    soup = BeautifulSoup(temp, 'html5lib')\n",
    "    paper_name = soup.find_all('h2', class_='sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR')\n",
    "    for i in range(0,len(paper_name)):\n",
    "        print(\"Paper Name \", i+1, \"is \",paper_name[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "af90b057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of cnbc website : https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
      "Paper Name  1 is  Reward is enough\n",
      "Paper Name  2 is  Making sense of raw input\n",
      "Paper Name  3 is  Law and logic: A review from an argumentation perspective\n",
      "Paper Name  4 is  Creativity and artificial intelligence\n",
      "Paper Name  5 is  Artificial cognition for social human–robot interaction: An implementation\n",
      "Paper Name  6 is  Explanation in artificial intelligence: Insights from the social sciences\n",
      "Paper Name  7 is  Making sense of sensory input\n",
      "Paper Name  8 is  Conflict-based search for optimal multi-agent pathfinding\n",
      "Paper Name  9 is  Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning\n",
      "Paper Name  10 is  The Hanabi challenge: A new frontier for AI research\n",
      "Paper Name  11 is  Evaluating XAI: A comparison of rule-based and example-based explanations\n",
      "Paper Name  12 is  Argumentation in artificial intelligence\n",
      "Paper Name  13 is  Algorithms for computing strategies in two-player simultaneous move games\n",
      "Paper Name  14 is  Multiple object tracking: A literature review\n",
      "Paper Name  15 is  Selection of relevant features and examples in machine learning\n",
      "Paper Name  16 is  A survey of inverse reinforcement learning: Challenges, methods and progress\n",
      "Paper Name  17 is  Explaining individual predictions when features are dependent: More accurate approximations to Shapley values\n",
      "Paper Name  18 is  A review of possible effects of cognitive biases on interpretation of rule-based machine learning models\n",
      "Paper Name  19 is  Integrating social power into the decision-making of cognitive agents\n",
      "Paper Name  20 is  “That's (not) the output I expected!” On the role of end user expectations in creating explanations of AI systems\n",
      "Paper Name  21 is  Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies\n",
      "Paper Name  22 is  Algorithm runtime prediction: Methods & evaluation\n",
      "Paper Name  23 is  Wrappers for feature subset selection\n",
      "Paper Name  24 is  Commonsense visual sensemaking for autonomous driving – On generalised neurosymbolic online abduction integrating vision and semantics\n",
      "Paper Name  25 is  Quantum computation, quantum theory and AI\n"
     ]
    }
   ],
   "source": [
    "journal_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9957b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer ii   https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "\n",
    "def journal_author():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of cnbc website : \")\n",
    "    page = requests.get(url)\n",
    "    temp = page.content\n",
    "    \n",
    "    soup = BeautifulSoup(temp, 'html5lib')\n",
    "    paper_name = soup.find_all('p', class_='sc-1thf9ly-0 sc-1thf9ly-1 ljBYfY bIzabL')\n",
    "    for i in range(0,len(paper_name)):\n",
    "        print(\"Paper Author Name \", i+1, \"is \",paper_name[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8bfcb69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of cnbc website : https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
      "Paper Author Name  1 is  Silver, David, Singh, Satinder, Precup, Doina, Sutton, Richard S. Open AccessOctober 2021\n",
      "Paper Author Name  2 is  Evans, Richard, Bošnjak, Matko and 5 moreOpen AccessOctober 2021\n",
      "Paper Author Name  3 is  Prakken, Henry, Sartor, Giovanni Open AccessOctober 2015\n",
      "Paper Author Name  4 is  Boden, Margaret A. Open AccessAugust 1998\n",
      "Paper Author Name  5 is  Lemaignan, Séverin, Warnier, Mathieu and 3 moreOpen AccessJune 2017\n",
      "Paper Author Name  6 is  Miller, Tim February 2019\n",
      "Paper Author Name  7 is  Evans, Richard, Hernández-Orallo, José and 3 moreOpen AccessApril 2021\n",
      "Paper Author Name  8 is  Sharon, Guni, Stern, Roni, Felner, Ariel, Sturtevant, Nathan R. Open AccessFebruary 2015\n",
      "Paper Author Name  9 is  Sutton, Richard S., Precup, Doina, Singh, Satinder Open AccessAugust 1999\n",
      "Paper Author Name  10 is  Bard, Nolan, Foerster, Jakob N. and 13 moreOpen AccessMarch 2020\n",
      "Paper Author Name  11 is  van der Waa, Jasper, Nieuwburg, Elisabeth, Cremers, Anita, Neerincx, Mark Open AccessFebruary 2021\n",
      "Paper Author Name  12 is  Bench-Capon, T.J.M., Dunne, Paul E. Open AccessOctober 2007\n",
      "Paper Author Name  13 is  Bošanský, Branislav, Lisý, Viliam and 3 moreOpen AccessAugust 2016\n",
      "Paper Author Name  14 is  Luo, Wenhan, Xing, Junliang and 4 moreApril 2021\n",
      "Paper Author Name  15 is  Blum, Avrim L., Langley, Pat Open AccessDecember 1997\n",
      "Paper Author Name  16 is  Arora, Saurabh, Doshi, Prashant August 2021\n",
      "Paper Author Name  17 is  Aas, Kjersti, Jullum, Martin, Løland, Anders Open AccessSeptember 2021\n",
      "Paper Author Name  18 is  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Johannes Open AccessJune 2021\n",
      "Paper Author Name  19 is  Pereira, Gonçalo, Prada, Rui, Santos, Pedro A. Open AccessDecember 2016\n",
      "Paper Author Name  20 is  Riveiro, Maria, Thill, Serge Open AccessSeptember 2021\n",
      "Paper Author Name  21 is  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, Keane, Mark T. Open AccessMay 2021\n",
      "Paper Author Name  22 is  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyton-Brown, Kevin Open AccessJanuary 2014\n",
      "Paper Author Name  23 is  Kohavi, Ron, John, George H. Open AccessDecember 1997\n",
      "Paper Author Name  24 is  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srikrishna Open AccessOctober 2021\n",
      "Paper Author Name  25 is  Ying, Mingsheng Open AccessFebruary 2010\n"
     ]
    }
   ],
   "source": [
    "journal_author()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "433b8a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer iii   https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "\n",
    "def journal_publish_date():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of cnbc website : \")\n",
    "    page = requests.get(url)\n",
    "    temp = page.content\n",
    "    \n",
    "    soup = BeautifulSoup(temp, 'html5lib')\n",
    "    paper_name = soup.find_all('span', class_='sc-1thf9ly-2 bKddwo')\n",
    "    for i in range(0,len(paper_name)):\n",
    "        print(\"Paper Publish Date of \", i+1, \"journal is \",paper_name[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9052a0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of cnbc website : https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
      "Paper Publish Date of  1 journal is  October 2021\n",
      "Paper Publish Date of  2 journal is  October 2021\n",
      "Paper Publish Date of  3 journal is  October 2015\n",
      "Paper Publish Date of  4 journal is  August 1998\n",
      "Paper Publish Date of  5 journal is  June 2017\n",
      "Paper Publish Date of  6 journal is  February 2019\n",
      "Paper Publish Date of  7 journal is  April 2021\n",
      "Paper Publish Date of  8 journal is  February 2015\n",
      "Paper Publish Date of  9 journal is  August 1999\n",
      "Paper Publish Date of  10 journal is  March 2020\n",
      "Paper Publish Date of  11 journal is  February 2021\n",
      "Paper Publish Date of  12 journal is  October 2007\n",
      "Paper Publish Date of  13 journal is  August 2016\n",
      "Paper Publish Date of  14 journal is  April 2021\n",
      "Paper Publish Date of  15 journal is  December 1997\n",
      "Paper Publish Date of  16 journal is  August 2021\n",
      "Paper Publish Date of  17 journal is  September 2021\n",
      "Paper Publish Date of  18 journal is  June 2021\n",
      "Paper Publish Date of  19 journal is  December 2016\n",
      "Paper Publish Date of  20 journal is  September 2021\n",
      "Paper Publish Date of  21 journal is  May 2021\n",
      "Paper Publish Date of  22 journal is  January 2014\n",
      "Paper Publish Date of  23 journal is  December 1997\n",
      "Paper Publish Date of  24 journal is  October 2021\n",
      "Paper Publish Date of  25 journal is  February 2010\n"
     ]
    }
   ],
   "source": [
    "journal_publish_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4291cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer iv   https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "\n",
    "def journal_url():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of cnbc website : \")\n",
    "    page = requests.get(url)\n",
    "    temp = page.content\n",
    "    \n",
    "    soup = BeautifulSoup(temp, 'html5lib')\n",
    "    paper_link = soup.find_all('li', class_='sc-9zxyh7-1 sc-9zxyh7-2 exAXfr jQmQZp')\n",
    "    for i in range(0,len(paper_link)):\n",
    "        a = paper_link[i].find('a')['href']\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "78da9936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of cnbc website : https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000862\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000722\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370215000910\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370298000551\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370216300790\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370218305988\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370220301855\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370214001386\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370299000521\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370219300116\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370220301533\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370207000793\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370216300285\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370220301958\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370297000635\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000515\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000539\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000096\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370216300868\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000588\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000102\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370213001082\n",
      "https://www.sciencedirect.com/science/article/pii/S000437029700043X\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000734\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370209001398\n"
     ]
    }
   ],
   "source": [
    "journal_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1396b258",
   "metadata": {},
   "source": [
    "# 9) Write a python program to scrape mentioned details from dineout.co.in :\n",
    "i) Restaurant name\n",
    "\n",
    "ii) Cuisine\n",
    "\n",
    "iii) Location\n",
    "\n",
    "iv) Ratings\n",
    "\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ab5245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.dineout.co.in/\n",
    "\n",
    "def restaurent_name():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of cnbc website : \")\n",
    "    page = requests.get(url)\n",
    "    temp = page.content\n",
    "    \n",
    "    soup = BeautifulSoup(temp, 'html5lib')\n",
    "    paper_name = soup.find_all('h4', class_='_1jbOb')\n",
    "    for i in range(0,len(paper_name)):\n",
    "        print(\"Restaurent Name is \",paper_name[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5d1971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of cnbc website : https://www.dineout.co.in/\n",
      "Restaurent Name is  Connaught Clubhouse Microbrewery\n",
      "Restaurent Name is  The Luggage Room By Sandoz\n",
      "Restaurent Name is  38 Barracks\n",
      "Restaurent Name is  Master Of Malts\n"
     ]
    }
   ],
   "source": [
    "restaurent_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aff3e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer ii   https://www.dineout.co.in/delhi-restaurants/buffet-special\n",
    "\n",
    "\n",
    "\n",
    "def restaurent_cuisine():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of cnbc website : \")\n",
    "    page = requests.get(url)\n",
    "     \n",
    "    soup=BeautifulSoup(page.content)\n",
    "    cuisine = []\n",
    "    for i in soup.find_all('span',class_=\"double-line-ellipsis\"):\n",
    "        cuisine.append(i.text)\n",
    "    print(cuisine)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0677411c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of cnbc website : https://www.dineout.co.in/delhi-restaurants/buffet-special\n",
      "['₹ 2,000 for 2 (approx) | Chinese, North Indian', '₹ 1,680 for 2 (approx) | North Indian, Asian, Italian', '₹ 2,000 for 2 (approx) | Chinese, North Indian', '₹ 3,000 for 2 (approx) | Italian, Continental', '₹ 1,700 for 2 (approx) | North Indian, Chinese', '₹ 2,400 for 2 (approx) | North Indian, Italian', '₹ 1,800 for 2 (approx) | North Indian', '₹ 1,900 for 2 (approx) | North Indian', '₹ 2,200 for 2 (approx) | North Indian, Mughlai']\n"
     ]
    }
   ],
   "source": [
    "restaurent_cuisine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2af11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer iii https://www.dineout.co.in/delhi-restaurants/italian-cuisine\n",
    "\n",
    "def restaurent_loc():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of cnbc website : \")\n",
    "    page = requests.get(url)\n",
    "    temp = page.content\n",
    "    \n",
    "    soup = BeautifulSoup(temp, 'html5lib')\n",
    "    loc = soup.find_all('div', class_='restnt-loc ellipsis')\n",
    "    for i in range(0,len(loc)):\n",
    "        print(\"Restaurent location is \",loc[i].find('a').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f7c8e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of cnbc website : https://www.dineout.co.in/delhi-restaurants/italian-cuisine\n",
      "Restaurent location is  Worldmark 2,\n",
      "Restaurent location is  Gwal Pahari\n",
      "Restaurent location is  Moti Bagh\n",
      "Restaurent location is  Ashok Vihar Phase - 3\n",
      "Restaurent location is  Sector 135\n",
      "Restaurent location is  Kalkaji\n",
      "Restaurent location is  Mehrauli\n",
      "Restaurent location is  PVR Plaza,\n",
      "Restaurent location is  The Leela Palace,\n",
      "Restaurent location is  ITC Maurya,\n",
      "Restaurent location is  The Claridges,\n",
      "Restaurent location is  JW Mariott Hotel New Delhi Aerocity,\n",
      "Restaurent location is  PVR Anupam Complex,\n",
      "Restaurent location is  Holiday Inn,\n",
      "Restaurent location is  Ansal Plaza Mall,\n",
      "Restaurent location is  DDA Complex,\n",
      "Restaurent location is  Rajouri Garden\n",
      "Restaurent location is  Mehar Chand Market,\n",
      "Restaurent location is  Karma Lakelands,\n",
      "Restaurent location is  Sector 22\n",
      "Restaurent location is  Dwarka\n"
     ]
    }
   ],
   "source": [
    "restaurent_loc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27485f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer iv https://www.dineout.co.in/delhi-restaurants/italian-cuisine\n",
    "\n",
    "def restaurent_ratings():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of cnbc website : \")\n",
    "    page = requests.get(url)\n",
    "    temp = page.content\n",
    "    \n",
    "    soup = BeautifulSoup(temp, 'html5lib')\n",
    "    loc = soup.find_all('div', class_='restnt-rating rating-4')\n",
    "    for i in range(0,len(loc)):\n",
    "        print(\"Restaurent Rating is \",loc[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fbb0ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of cnbc website : https://www.dineout.co.in/delhi-restaurants/italian-cuisine\n",
      "Restaurent Rating is  4.1\n",
      "Restaurent Rating is  4.4\n",
      "Restaurent Rating is  4.1\n",
      "Restaurent Rating is  3.9\n",
      "Restaurent Rating is  4.4\n",
      "Restaurent Rating is  4.3\n"
     ]
    }
   ],
   "source": [
    "restaurent_ratings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb981f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer v https://www.dineout.co.in/delhi-restaurants/italian-cuisine\n",
    "\n",
    "def restaurent_url():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of cnbc website : \")\n",
    "    page = requests.get(url)\n",
    "    temp = page.content\n",
    "    \n",
    "    soup = BeautifulSoup(temp)\n",
    "    images = []\n",
    "    for i in soup.find_all('img',class_=\"no-img\"):\n",
    "        images.append(i['data-src'])\n",
    "\n",
    "    for i in range(0,len(images)):\n",
    "        print(images[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f90eea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of cnbc website : https://www.dineout.co.in/delhi-restaurants/italian-cuisine\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/q/v/p58421-16295734196121512be5310.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/8/l/o/p83352-16003243605f630308a83a0.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/1/h/a/p102204-1637666194619ccd9270be0.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/1/a/g/p103764-1644402367620396bf5bd91.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/1/h/g/p103818-16455363136214e4394b104.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/1/q/e/p104167-16457083476217843b3c57c.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/7/j/e/p78-15595552945cf4ecde54f95.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/1/o/b/p1317-15164263275a62d457c112d.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/1/v/s/p1500-14648753265750393ebee33.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/r/p/p2917-14639996715742dcb75b8e2.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/s/n/p2946-15854614295e8038b5181b6.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/1/g/i/p12809-144948765956656d2b0007f.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/h/y/p21966-1467101510577231468e696.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/d/i/p22309-147435905257e0ef0c57ba6.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/t/y/p28580-1637582627619b872315619.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/w/d/p35287-15141992665a40d8e261cbe.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/c/k/p37962-15268826035b02612bcd1b8.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/4/q/y/p4837-15656920305d52907edc23e.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/v/r/p51539-16085429365fe06ad89691d.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/v/p/p54570-15535100425c98ae9a6d9e2.jpg?tr=tr:n-medium\n",
      "https://im1.dineout.co.in/images/uploads/restaurant/sharpen/7/j/t/p77743-162667596760f51aff32c82.jpg?tr=tr:n-medium\n"
     ]
    }
   ],
   "source": [
    "restaurent_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1184e0c7",
   "metadata": {},
   "source": [
    "# 10) Write a python program to scrape the details of top publications from Google Scholar from\n",
    "https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "    \n",
    "i) Rank\n",
    "\n",
    "ii) Publication\n",
    "\n",
    "iii) h5-index\n",
    "\n",
    "iv) h5-median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9473f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer     https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "\n",
    "def google_scholar():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    url = input(\"Enter the name of Google Scholar website : \")\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    res = soup.find_all('td')\n",
    "\n",
    "    gs = []\n",
    "\n",
    "    for i in range(0,len(res),4):\n",
    "        n1 = res[i].text.strip()\n",
    "        n2 = res[i+1].text.strip()\n",
    "        n3 = res[i+2].text.strip()\n",
    "        n4 = res[i+3].text.strip()\n",
    "        gs.append([n1, n2, n3, n4])\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        print(gs[i])\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data=gs,columns=[\"Rank\", \"Publication\", \"h5-index\", \"h5-median\"])\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6f6d032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of Google Scholar website : https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
      "['1.', 'Nature', '444', '667']\n",
      "['2.', 'The New England Journal of Medicine', '432', '780']\n",
      "['3.', 'Science', '401', '614']\n",
      "['4.', 'IEEE/CVF Conference on Computer Vision and Pattern Recognition', '389', '627']\n",
      "['5.', 'The Lancet', '354', '635']\n",
      "['6.', 'Advanced Materials', '312', '418']\n",
      "['7.', 'Nature Communications', '307', '428']\n",
      "['8.', 'Cell', '300', '505']\n",
      "['9.', 'International Conference on Learning Representations', '286', '533']\n",
      "['10.', 'Neural Information Processing Systems', '278', '436']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Publication</th>\n",
       "      <th>h5-index</th>\n",
       "      <th>h5-median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>432</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Science</td>\n",
       "      <td>401</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>389</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>354</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>145</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>145</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>144</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>144</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                        Publication h5-index h5-median\n",
       "0     1.                                             Nature      444       667\n",
       "1     2.                The New England Journal of Medicine      432       780\n",
       "2     3.                                            Science      401       614\n",
       "3     4.  IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
       "4     5.                                         The Lancet      354       635\n",
       "..   ...                                                ...      ...       ...\n",
       "95   96.                       Journal of Business Research      145       233\n",
       "96   97.                                   Molecular Cancer      145       209\n",
       "97   98.                                            Sensors      145       201\n",
       "98   99.                              Nature Climate Change      144       228\n",
       "99  100.                    IEEE Internet of Things Journal      144       212\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = google_scholar()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d906e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
