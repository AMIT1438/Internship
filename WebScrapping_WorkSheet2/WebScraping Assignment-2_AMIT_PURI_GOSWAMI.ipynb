{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba2c1c3",
   "metadata": {},
   "source": [
    "   # WEB SCRAPING – ASSIGNMENT 2\n",
    "   \n",
    "1. All the questions must be done in a single Jupyter notebook.\n",
    "2. There should be proper comments in code.\n",
    "\n",
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a2b54c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ca1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c114ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening naukri page on authomated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeb97160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering designation and location as required in the question\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Analyst')\n",
    "\n",
    "location = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys('Bangalore')\n",
    "\n",
    "#location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\").send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a64ef2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking the submit button\n",
    "search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59acd070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring 4 empty lists to store job title, location , name and experience to be scrapped fromt the web page\n",
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "experience_required = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "214382a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping jobtitle from the given page\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "#scraping joblocation from the given page\n",
    "location_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_location.append(title)\n",
    "    \n",
    " #scraping company from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    title=i.text\n",
    "    company_name.append(title)\n",
    "    \n",
    "#scraping job Experience from the given page\n",
    "experience_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in experience_tags[0:10]:\n",
    "    title=i.text\n",
    "    experience_required.append(title)\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a86c8ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "#To find lengths of the lists\n",
    "print(len(job_title), len(job_location), len(company_name), len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30d6bb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>job_location</th>\n",
       "      <th>company_name</th>\n",
       "      <th>experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru(Old Madras Road)</td>\n",
       "      <td>KrazyBee</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sr. Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, Pune</td>\n",
       "      <td>Global Indian School Education Services</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Master Data Management Business Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Optum</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Intern Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>FullStackTechies</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst - Python/Artificial Intelligence</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>iMindYourBusiness</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lead Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>McAfee</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Bayer</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Leap COE Intern(Data Analyst)</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Info Origin Inc.</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Forecasting Analyst/ Data Scientist (US Client)</td>\n",
       "      <td>Bangalore/Bengaluru, Gurgaon/Gurugram\\n(WFH du...</td>\n",
       "      <td>Concentrix</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Job Title  \\\n",
       "0                              Senior Data Analyst   \n",
       "1                                 Sr. Data Analyst   \n",
       "2          Master Data Management Business Analyst   \n",
       "3                              Senior Data Analyst   \n",
       "4                              Intern Data Analyst   \n",
       "5    Data Analyst - Python/Artificial Intelligence   \n",
       "6                                Lead Data Analyst   \n",
       "7                                     Data Analyst   \n",
       "8                    Leap COE Intern(Data Analyst)   \n",
       "9  Forecasting Analyst/ Data Scientist (US Client)   \n",
       "\n",
       "                                        job_location  \\\n",
       "0               Bangalore/Bengaluru(Old Madras Road)   \n",
       "1                          Bangalore/Bengaluru, Pune   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9  Bangalore/Bengaluru, Gurgaon/Gurugram\\n(WFH du...   \n",
       "\n",
       "                              company_name experience_required  \n",
       "0                                 KrazyBee             3-6 Yrs  \n",
       "1  Global Indian School Education Services            6-11 Yrs  \n",
       "2                                Accenture             6-8 Yrs  \n",
       "3                                    Optum             5-7 Yrs  \n",
       "4                         FullStackTechies             0-1 Yrs  \n",
       "5                        iMindYourBusiness             0-2 Yrs  \n",
       "6                                   McAfee             5-9 Yrs  \n",
       "7                                    Bayer             2-5 Yrs  \n",
       "8                         Info Origin Inc.             0-1 Yrs  \n",
       "9                               Concentrix             3-8 Yrs  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating data frome and displaying it\n",
    "df=pd.DataFrame({'Job Title':job_title,'job_location':job_location,'company_name':company_name,'experience_required':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4353b3f",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "    \n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7161d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_naukariscrapping():\n",
    "    import selenium\n",
    "    import pandas as pd\n",
    "    from selenium import webdriver\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    from selenium.webdriver.common.by import By\n",
    "    import time\n",
    "    \n",
    "    #Connect to the driver\n",
    "    driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "    \n",
    "    #opening naukri page on authomated chrome browser\n",
    "    driver.get(\"https://www.naukri.com/\")\n",
    "    \n",
    "    #Entering designation and location as required in the question\n",
    "    designation = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "    designation.send_keys('Data Scientist')\n",
    "\n",
    "    location = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\")\n",
    "    location.send_keys('Bangalore')\n",
    "    \n",
    "    #Clicking the submit button\n",
    "    search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "    search.click()\n",
    "    time.sleep(5)\n",
    "   \n",
    "    #Declaring 3 empty lists to store job title, location and name to be scrapped fromt the web page\n",
    "    job_title = []\n",
    "    job_location = []\n",
    "    company_name = []\n",
    "\n",
    "    #scraping jobtitle from the given page\n",
    "    title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "    for i in title_tags[0:10]:\n",
    "        title=i.text\n",
    "        job_title.append(title)\n",
    "    \n",
    "    #scraping joblocation from the given page\n",
    "    location_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "    for i in location_tags[0:10]:\n",
    "        title=i.text\n",
    "        job_location.append(title)\n",
    "    \n",
    "     #scraping company from the given page\n",
    "    company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "    for i in company_tags[0:10]:\n",
    "        title=i.text\n",
    "        company_name.append(title)\n",
    "        \n",
    "    #scraping job Experience from the given page\n",
    "experience_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in experience_tags[0:10]:\n",
    "    title=i.text\n",
    "    experience_required.append(title)\n",
    "    \n",
    "    #creating data frome and displaying it\n",
    "    df=pd.DataFrame({'Job Title':job_title,'job_location':job_location,'company_name':company_name})\n",
    "    return(df)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83649fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>job_location</th>\n",
       "      <th>company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tcs Hiring For Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Chennai, Mumbai (All Areas)</td>\n",
       "      <td>TATA CONSULTANCY SERVICES (TCS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Manager - EmTech - Machine Learning - P&amp;T</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>PwC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data scientist _Tata Consultancy Services(Tcs)</td>\n",
       "      <td>Bangalore/Bengaluru, Kochi/Cochin, Indore, New...</td>\n",
       "      <td>TATA CONSULTANCY SERVICES (TCS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Job||Job Opening For AI Technologist - Data Sc...</td>\n",
       "      <td>Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...</td>\n",
       "      <td>Wipro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hiring For DATA Scientist @ NTT DATA Business ...</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "      <td>NTT DATA Business Solutions Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Urgent Job Opening For AI Practitioner - Data ...</td>\n",
       "      <td>Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...</td>\n",
       "      <td>Wipro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist Python</td>\n",
       "      <td>Bangalore/Bengaluru, Hyderabad/Secunderabad</td>\n",
       "      <td>Conduent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Assistant Manager - Data Science</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune</td>\n",
       "      <td>CitiusTech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, New Delhi, Pune, Gurgaon/...</td>\n",
       "      <td>ZS Associates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0                      Tcs Hiring For Data Scientist   \n",
       "1   Senior Manager - EmTech - Machine Learning - P&T   \n",
       "2                   Analystics & Modeling Specialist   \n",
       "3     Data scientist _Tata Consultancy Services(Tcs)   \n",
       "4  Job||Job Opening For AI Technologist - Data Sc...   \n",
       "5  Hiring For DATA Scientist @ NTT DATA Business ...   \n",
       "6  Urgent Job Opening For AI Practitioner - Data ...   \n",
       "7                              Data Scientist Python   \n",
       "8                   Assistant Manager - Data Science   \n",
       "9                                     Data Scientist   \n",
       "\n",
       "                                        job_location  \\\n",
       "0   Bangalore/Bengaluru, Chennai, Mumbai (All Areas)   \n",
       "1  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...   \n",
       "2  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...   \n",
       "3  Bangalore/Bengaluru, Kochi/Cochin, Indore, New...   \n",
       "4  Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...   \n",
       "5  Bangalore/Bengaluru, Noida, Hyderabad/Secunder...   \n",
       "6  Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...   \n",
       "7        Bangalore/Bengaluru, Hyderabad/Secunderabad   \n",
       "8                  Bangalore/Bengaluru, Mumbai, Pune   \n",
       "9  Bangalore/Bengaluru, New Delhi, Pune, Gurgaon/...   \n",
       "\n",
       "                                  company_name  \n",
       "0              TATA CONSULTANCY SERVICES (TCS)  \n",
       "1                                          PwC  \n",
       "2                                    Accenture  \n",
       "3              TATA CONSULTANCY SERVICES (TCS)  \n",
       "4                                        Wipro  \n",
       "5  NTT DATA Business Solutions Private Limited  \n",
       "6                                        Wipro  \n",
       "7                                     Conduent  \n",
       "8                                   CitiusTech  \n",
       "9                                ZS Associates  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=get_naukariscrapping()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baf7d25",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "\n",
    "\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa3b3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Connect to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#opening naukri page on authomated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "#Entering designation and location as required in the question\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Scientist')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f66cfde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking the submit button\n",
    "search = driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd8ed9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking the Delhi / NCR check box\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[5]/div[2]/div[3]/label/i\")\n",
    "search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "162a131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking the 3-6 Lakhs check box under salary\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[6]/div[2]/div[2]/label/p/span[1]\")\n",
    "search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca5c3443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>job_location</th>\n",
       "      <th>company_name</th>\n",
       "      <th>experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job||Job Opening For AI Technologist - Data Sc...</td>\n",
       "      <td>New Delhi, Hyderabad/Secunderabad, Pune, Chenn...</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist - Noida/Bangalore</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "      <td>EXL</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DigitalBCG GAMMA Data Scientist</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - Predictive Analytics</td>\n",
       "      <td>Noida, Mumbai, Chandigarh, Hyderabad/Secundera...</td>\n",
       "      <td>Confidential</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Optum</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chat-bot Developer / Data Scientist</td>\n",
       "      <td>Mumbai, New Delhi, Bangalore/Bengaluru\\n(WFH d...</td>\n",
       "      <td>Big Seo Buzz</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist / Chat-bot Developer</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru, Mumbai (All Ar...</td>\n",
       "      <td>Big Seo Buzz</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Primo Hiring</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist For Healthcare Product team</td>\n",
       "      <td>Delhi / NCR, Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>SECUREKLOUD TECHNOLOGIES</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida, Gurgaon/Gurugram, Jaipur</td>\n",
       "      <td>Celebal Technologies</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0  Job||Job Opening For AI Technologist - Data Sc...   \n",
       "1                   Data Scientist - Noida/Bangalore   \n",
       "2                    DigitalBCG GAMMA Data Scientist   \n",
       "3              Data Scientist - Predictive Analytics   \n",
       "4                                     Data Scientist   \n",
       "5                Chat-bot Developer / Data Scientist   \n",
       "6                Data Scientist / Chat-bot Developer   \n",
       "7                  Data Scientist - Engine Algorithm   \n",
       "8         Data Scientist For Healthcare Product team   \n",
       "9                                     Data Scientist   \n",
       "\n",
       "                                        job_location  \\\n",
       "0  New Delhi, Hyderabad/Secunderabad, Pune, Chenn...   \n",
       "1                         Noida, Bangalore/Bengaluru   \n",
       "2                     New Delhi, Bangalore/Bengaluru   \n",
       "3  Noida, Mumbai, Chandigarh, Hyderabad/Secundera...   \n",
       "4                                   Gurgaon/Gurugram   \n",
       "5  Mumbai, New Delhi, Bangalore/Bengaluru\\n(WFH d...   \n",
       "6  New Delhi, Bangalore/Bengaluru, Mumbai (All Ar...   \n",
       "7  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "8          Delhi / NCR, Chennai, Bangalore/Bengaluru   \n",
       "9                    Noida, Gurgaon/Gurugram, Jaipur   \n",
       "\n",
       "               company_name experience_required  \n",
       "0                     Wipro            5-10 Yrs  \n",
       "1                       EXL            5-10 Yrs  \n",
       "2   Boston Consulting Group             2-5 Yrs  \n",
       "3              Confidential             1-6 Yrs  \n",
       "4                     Optum             2-7 Yrs  \n",
       "5              Big Seo Buzz             2-7 Yrs  \n",
       "6              Big Seo Buzz             3-7 Yrs  \n",
       "7              Primo Hiring             1-3 Yrs  \n",
       "8  SECUREKLOUD TECHNOLOGIES             2-7 Yrs  \n",
       "9      Celebal Technologies             1-5 Yrs  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Declaring 4 empty lists to store job title, location and name to be scrapped fromt the web page\n",
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "experience_required = []\n",
    "\n",
    "#scraping jobtitle from the given page\n",
    "try:\n",
    "    title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "    for i in title_tags[0:10]:\n",
    "        title=i.text\n",
    "        job_title.append(title)\n",
    "except NoSuchElementException:  #spelling error making this code not work as expected\n",
    "    pass\n",
    "\n",
    "#scraping joblocation from the given page\n",
    "location_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_location.append(title)\n",
    "\n",
    "#scraping company from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    title=i.text\n",
    "    company_name.append(title)\n",
    "    \n",
    "\n",
    "#scraping job Experience from the given page\n",
    "experience_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in experience_tags[0:10]:\n",
    "    title=i.text\n",
    "    experience_required.append(title)\n",
    "    \n",
    "\n",
    "#creating data frome and displaying it\n",
    "df=pd.DataFrame({'Job Title':job_title,'job_location':job_location,'company_name':company_name,'experience_required':experience_required})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfccdb23",
   "metadata": {},
   "source": [
    "# Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "ASSIGNMENT 2\n",
    "\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses.\n",
    "\n",
    "Note: That all of the above steps have to be done by coding only and not manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "491825c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Connect to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#opening flipkart page on authomated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f96e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering sunglasses in the search box\n",
    "article = driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "article.send_keys('sunglasses')\n",
    "\n",
    "#Clicking the search icon\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6849c56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured\n"
     ]
    }
   ],
   "source": [
    "#Declaring 3 empty lists for storing Brand, product description and price\n",
    "brand = []\n",
    "prod_desc = []\n",
    "price = []\n",
    "\n",
    "try:\n",
    "    for i in range(1,5):\n",
    "      \n",
    "        brand_tag=driver.find_elements(By.CLASS_NAME,\"_2WkVRV\")\n",
    "        prod_tag=driver.find_elements(By.CLASS_NAME,\"IRpwTa\")\n",
    "        price_tag=driver.find_elements(By.CLASS_NAME,\"_3bPFwb\")\n",
    "        \n",
    "        time.sleep(10)\n",
    "        \n",
    "        for i in brand_tag:\n",
    "            brand.append(i.text)\n",
    "        \n",
    "        for i in prod_tag:\n",
    "            prod_desc.append(i.text)\n",
    "        \n",
    "        for i in price_tag:\n",
    "            price.append(i.text)\n",
    "        \n",
    "        #Clicking on next page and scraping other detail\n",
    "        nextpage=driver.find_element(By.CLASS_NAME, '_1LKTO3')\n",
    "        nextpage.click()\n",
    "    \n",
    "        if(len(brand)<100):\n",
    "            continue\n",
    "except:\n",
    "    print(\"An error occured\")\n",
    "finally:\n",
    "    #creating data frome and displaying it\n",
    "    df=pd.DataFrame({'Brand':brand,'Product Description':prod_desc,'Price':price})\n",
    "    df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "538ab6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GANSTA</td>\n",
       "      <td>Night Vision, Riding Glasses Rectangular Sungl...</td>\n",
       "      <td>₹242₹1,29981% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Mirrored Aviator Sunglasses (58)</td>\n",
       "      <td>₹379₹1,49974% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>Mirrored, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹269₹1,49982% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Wayfarer ...</td>\n",
       "      <td>₹749₹2,50070% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹759₹89915% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹279₹1,59982% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Gradient, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹679₹79915% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>john jacobs</td>\n",
       "      <td>UV Protection Round Sunglasses (53)</td>\n",
       "      <td>₹3,325₹5,00033% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Retro Square Sunglasse...</td>\n",
       "      <td>₹349₹1,99982% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹189₹1,59988% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Round Sunglasses (Free...</td>\n",
       "      <td>₹399₹1,99980% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart UV Protection Aviator Sunglasses (53)</td>\n",
       "      <td>₹1,229₹1,99938% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹669₹89925% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Aviator Sunglasses (55)</td>\n",
       "      <td>₹649₹2,55574% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sewell</td>\n",
       "      <td>Mirrored, Night Vision, UV Protection, Riding ...</td>\n",
       "      <td>₹279₹1,49981% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Aviator S...</td>\n",
       "      <td>₹999₹1,99950% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>Polarized, UV Protection Rectangular Sunglasse...</td>\n",
       "      <td>₹949₹2,50062% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (53)</td>\n",
       "      <td>₹235₹99876% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>EYELLUSION</td>\n",
       "      <td>Riding Glasses, Riding Glasses, UV Protection ...</td>\n",
       "      <td>₹199₹99980% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Aviator S...</td>\n",
       "      <td>₹1,049₹1,99947% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Mirrored Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹379₹1,49974% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Wayfarer ...</td>\n",
       "      <td>₹849₹1,99957% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GANSTA</td>\n",
       "      <td>Gradient, UV Protection Aviator Sunglasses (57)</td>\n",
       "      <td>₹314₹1,99984% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Aviator S...</td>\n",
       "      <td>₹1,415₹1,99929% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>UV Protection Round Sunglasses (Free Size)</td>\n",
       "      <td>₹268₹1,29979% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹214₹99978% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Lee Topper</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹219₹99978% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Cat-eye S...</td>\n",
       "      <td>₹849₹1,99957% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>Polarized, UV Protection Round Sunglasses (50)</td>\n",
       "      <td>₹949₹1,99952% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AISLIN</td>\n",
       "      <td>UV Protection, Gradient Butterfly, Over-sized ...</td>\n",
       "      <td>₹675₹2,60074% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Polarized, UV Protection Wayfarer, Retro Squar...</td>\n",
       "      <td>₹664₹1,99966% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Rectangul...</td>\n",
       "      <td>₹749₹1,99962% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Mirrored Aviator Sunglasses (55)</td>\n",
       "      <td>₹379₹1,49974% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>Mirrored, UV Protection Wayfarer, Rectangular ...</td>\n",
       "      <td>₹289₹1,99985% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Over-sized Sunglasses (65)</td>\n",
       "      <td>₹410₹2,59984% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Aviator S...</td>\n",
       "      <td>₹1,415₹1,99929% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Rich Club</td>\n",
       "      <td>UV Protection Round Sunglasses (50)</td>\n",
       "      <td>₹399₹99960% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (53)</td>\n",
       "      <td>₹207₹99879% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection Oval Sunglasses (Free Size)</td>\n",
       "      <td>₹208₹99979% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>CRYSTAL CART</td>\n",
       "      <td>Polarized, UV Protection, Gradient, Riding Gla...</td>\n",
       "      <td>₹349₹1,49976% off\\nFree delivery</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                Product Description  \\\n",
       "0           GANSTA  Night Vision, Riding Glasses Rectangular Sungl...   \n",
       "1        ROYAL SON                   Mirrored Aviator Sunglasses (58)   \n",
       "2   kingsunglasses  Mirrored, UV Protection Wayfarer Sunglasses (F...   \n",
       "3    VINCENT CHASE  by Lenskart Polarized, UV Protection Wayfarer ...   \n",
       "4         Fastrack      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "5           PIRASO              UV Protection Aviator Sunglasses (54)   \n",
       "6         Fastrack  Gradient, UV Protection Wayfarer Sunglasses (F...   \n",
       "7      john jacobs                UV Protection Round Sunglasses (53)   \n",
       "8   ROZZETTA CRAFT  UV Protection, Gradient Retro Square Sunglasse...   \n",
       "9        New Specs   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "10  ROZZETTA CRAFT  UV Protection, Gradient Round Sunglasses (Free...   \n",
       "11   VINCENT CHASE  by Lenskart UV Protection Aviator Sunglasses (53)   \n",
       "12        Fastrack       UV Protection Aviator Sunglasses (Free Size)   \n",
       "13  ROZZETTA CRAFT              UV Protection Aviator Sunglasses (55)   \n",
       "14          Sewell  Mirrored, Night Vision, UV Protection, Riding ...   \n",
       "15   VINCENT CHASE  by Lenskart Polarized, UV Protection Aviator S...   \n",
       "16   VINCENT CHASE  Polarized, UV Protection Rectangular Sunglasse...   \n",
       "17            SRPM             UV Protection Wayfarer Sunglasses (53)   \n",
       "18      EYELLUSION  Riding Glasses, Riding Glasses, UV Protection ...   \n",
       "19   VINCENT CHASE  by Lenskart Polarized, UV Protection Aviator S...   \n",
       "20       ROYAL SON            Mirrored Aviator Sunglasses (Free Size)   \n",
       "21   VINCENT CHASE  by Lenskart Polarized, UV Protection Wayfarer ...   \n",
       "22          GANSTA    Gradient, UV Protection Aviator Sunglasses (57)   \n",
       "23   VINCENT CHASE  by Lenskart Polarized, UV Protection Aviator S...   \n",
       "24       New Specs         UV Protection Round Sunglasses (Free Size)   \n",
       "25  kingsunglasses                UV Protection Round Sunglasses (54)   \n",
       "26      Lee Topper   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "27   VINCENT CHASE  by Lenskart Polarized, UV Protection Cat-eye S...   \n",
       "28   VINCENT CHASE     Polarized, UV Protection Round Sunglasses (50)   \n",
       "29          AISLIN  UV Protection, Gradient Butterfly, Over-sized ...   \n",
       "30       ROYAL SON  Polarized, UV Protection Wayfarer, Retro Squar...   \n",
       "31   VINCENT CHASE  by Lenskart Polarized, UV Protection Rectangul...   \n",
       "32       ROYAL SON                   Mirrored Aviator Sunglasses (55)   \n",
       "33  kingsunglasses  Mirrored, UV Protection Wayfarer, Rectangular ...   \n",
       "34          PIRASO           UV Protection Over-sized Sunglasses (65)   \n",
       "35   VINCENT CHASE  by Lenskart Polarized, UV Protection Aviator S...   \n",
       "36       Rich Club                UV Protection Round Sunglasses (50)   \n",
       "37       Elligator                UV Protection Round Sunglasses (53)   \n",
       "38  kingsunglasses          UV Protection Oval Sunglasses (Free Size)   \n",
       "39    CRYSTAL CART  Polarized, UV Protection, Gradient, Riding Gla...   \n",
       "\n",
       "                                 Price  \n",
       "0     ₹242₹1,29981% off\\nFree delivery  \n",
       "1     ₹379₹1,49974% off\\nFree delivery  \n",
       "2     ₹269₹1,49982% off\\nFree delivery  \n",
       "3     ₹749₹2,50070% off\\nFree delivery  \n",
       "4       ₹759₹89915% off\\nFree delivery  \n",
       "5     ₹279₹1,59982% off\\nFree delivery  \n",
       "6                      ₹679₹79915% off  \n",
       "7   ₹3,325₹5,00033% off\\nFree delivery  \n",
       "8     ₹349₹1,99982% off\\nFree delivery  \n",
       "9     ₹189₹1,59988% off\\nFree delivery  \n",
       "10                   ₹399₹1,99980% off  \n",
       "11  ₹1,229₹1,99938% off\\nFree delivery  \n",
       "12      ₹669₹89925% off\\nFree delivery  \n",
       "13    ₹649₹2,55574% off\\nFree delivery  \n",
       "14    ₹279₹1,49981% off\\nFree delivery  \n",
       "15    ₹999₹1,99950% off\\nFree delivery  \n",
       "16    ₹949₹2,50062% off\\nFree delivery  \n",
       "17      ₹235₹99876% off\\nFree delivery  \n",
       "18      ₹199₹99980% off\\nFree delivery  \n",
       "19  ₹1,049₹1,99947% off\\nFree delivery  \n",
       "20    ₹379₹1,49974% off\\nFree delivery  \n",
       "21    ₹849₹1,99957% off\\nFree delivery  \n",
       "22                   ₹314₹1,99984% off  \n",
       "23  ₹1,415₹1,99929% off\\nFree delivery  \n",
       "24    ₹268₹1,29979% off\\nFree delivery  \n",
       "25      ₹214₹99978% off\\nFree delivery  \n",
       "26      ₹219₹99978% off\\nFree delivery  \n",
       "27    ₹849₹1,99957% off\\nFree delivery  \n",
       "28    ₹949₹1,99952% off\\nFree delivery  \n",
       "29    ₹675₹2,60074% off\\nFree delivery  \n",
       "30    ₹664₹1,99966% off\\nFree delivery  \n",
       "31    ₹749₹1,99962% off\\nFree delivery  \n",
       "32    ₹379₹1,49974% off\\nFree delivery  \n",
       "33    ₹289₹1,99985% off\\nFree delivery  \n",
       "34    ₹410₹2,59984% off\\nFree delivery  \n",
       "35  ₹1,415₹1,99929% off\\nFree delivery  \n",
       "36      ₹399₹99960% off\\nFree delivery  \n",
       "37      ₹207₹99879% off\\nFree delivery  \n",
       "38      ₹208₹99979% off\\nFree delivery  \n",
       "39    ₹349₹1,49976% off\\nFree delivery  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Declaring 3 empty lists for storing Brand, product description and price\n",
    "brand = []\n",
    "prod_desc = []\n",
    "price = []\n",
    "\n",
    "try:\n",
    "    for i in range(1,5):\n",
    "      \n",
    "        brand_tag=driver.find_elements(By.CLASS_NAME,\"_2WkVRV\")\n",
    "        prod_tag=driver.find_elements(By.CLASS_NAME,\"IRpwTa\")\n",
    "        price_tag=driver.find_elements(By.CLASS_NAME,\"_3bPFwb\")\n",
    "        \n",
    "        time.sleep(10)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in brand_tag:\n",
    "            brand.append(i.text)\n",
    "            if(len(brand)>98):\n",
    "                break\n",
    "         \n",
    "        \n",
    "        for i in prod_tag:\n",
    "            prod_desc.append(i.text)\n",
    "            if(len(prod_desc)>98):\n",
    "                break\n",
    "        \n",
    "        for i in price_tag:\n",
    "            price.append(i.text)\n",
    "            if(len(price)>98):\n",
    "                break\n",
    "        \n",
    "        #Clicking on next page and scraping other detail\n",
    "        nextpage=driver.find_element(By.CLASS_NAME, '_1LKTO3')\n",
    "        nextpage.click()\n",
    "    \n",
    "        if(len(brand)<100):\n",
    "            continue\n",
    "except:\n",
    "    print(\"An error occured\")\n",
    "finally:\n",
    "    #creating data frome and displaying it\n",
    "    df=pd.DataFrame({'Brand':brand,'Product Description':prod_desc,'Price':price})\n",
    "    df    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1f6b2",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "\n",
    "This task will be done in following steps:\n",
    "\n",
    "1. First get the webpage https://www.flipkart.com/\n",
    "2. Enter “iphone 11” in “Search” field .\n",
    "3. Then click the search button.\n",
    "\n",
    "You will reach to the below shown webpage .\n",
    "\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbd7caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Connect to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#opening flipkart page on authomated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e80b0282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering iphone 11 in search box\n",
    "article = driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "article.send_keys('iphone 11')\n",
    "\n",
    "#Clicking the search icon\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e211fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on review\n",
    "nextpage=driver.find_element(By.CLASS_NAME, '_1lRcqv')\n",
    "nextpage.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2492cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eea7ec14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured\n"
     ]
    }
   ],
   "source": [
    "#Declaring 3 empty lists for storing rating, review and full review\n",
    "\n",
    "\n",
    "rating = []\n",
    "review = []\n",
    "full_review = []\n",
    "\n",
    "try:\n",
    "    for i in range(1,5):\n",
    "      \n",
    "        rating_tag=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "        review_tag=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "        fr_tag=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        #Storing rating in rating list     \n",
    "        for i in rating_tag:\n",
    "            rating.append(i.text)\n",
    "            if(len(rating)>98):\n",
    "                break\n",
    "         \n",
    "        #Storing review in revoew list\n",
    "        for i in review_tag:\n",
    "            review.append(i.text)\n",
    "            if(len(review)>98):\n",
    "                break\n",
    "        \n",
    "        #Storing full review in full_review list\n",
    "        for i in fr_tag:\n",
    "            full_review.append(i.text)\n",
    "            if(len(full_review)>98):\n",
    "                break\n",
    "        \n",
    "        #Clicking on next page and scraping other detail\n",
    "        nextpage=driver.find_element(By.CLASS_NAME, '_1LKTO3')\n",
    "        nextpage.click()\n",
    "    \n",
    "        #loop continues if 100 elements are not stored in ratings\n",
    "        if(len(rating)<100):\n",
    "            continue\n",
    "except:\n",
    "    print(\"An error occured\")\n",
    "finally:\n",
    "    #creating data frome and displaying it\n",
    "    df=pd.DataFrame({'Rating':rating,'Review':review,'Full Review':full_review})\n",
    "    df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7221ea5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>What a camera .....just awesome ..you can feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.\\n\\nI’m am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>i11 is worthy to buy, too much happy with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>It's my first time to use iOS phone and I am l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>Good choice</td>\n",
       "      <td>So far it’s been an AMAZING experience coming ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rating               Review  \\\n",
       "0      5       Simply awesome   \n",
       "1      5     Perfect product!   \n",
       "2      5  Best in the market!   \n",
       "3      5    Worth every penny   \n",
       "4      5   Highly recommended   \n",
       "5      5            Fabulous!   \n",
       "6      5        Great product   \n",
       "7      5    Worth every penny   \n",
       "8      5   Highly recommended   \n",
       "9      4          Good choice   \n",
       "\n",
       "                                         Full Review  \n",
       "0  Really satisfied with the Product I received.....  \n",
       "1  Amazing phone with great cameras and better ba...  \n",
       "2  Great iPhone very snappy experience as apple k...  \n",
       "3  Previously I was using one plus 3t it was a gr...  \n",
       "4  What a camera .....just awesome ..you can feel...  \n",
       "5  This is my first iOS phone. I am very happy wi...  \n",
       "6  Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
       "7  i11 is worthy to buy, too much happy with the ...  \n",
       "8  It's my first time to use iOS phone and I am l...  \n",
       "9  So far it’s been an AMAZING experience coming ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50437c8",
   "metadata": {},
   "source": [
    "# Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "    \n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "\n",
    "As shown in the below image, you have to scrape the tick marked attributes.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8c02d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Connect to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#opening flipkart page on authomated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "#Clicking the close button in loging box and closing it \n",
    "search = driver.find_element(By.XPATH,'//button[@class=\"_2KpZ6l _2doB4z\"]')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ba41700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering sneakers in the search box\n",
    "article = driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "article.send_keys('sneakers')\n",
    "\n",
    "#Clicking the search icon\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aa48ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = []\n",
    "prod_desc = []\n",
    "price = []\n",
    "\n",
    "def print_100sneakers():\n",
    "    #finding three elements brand, product description and price\n",
    "    brand_tag=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    prdesc_tag=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    price_tag=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "        \n",
    "        #requesting the website to not change elements for the given seconds\n",
    "    time.sleep(1)\n",
    "        \n",
    "             \n",
    "         #Storing brand in brand list     \n",
    "    for i in brand_tag:\n",
    "        brand.append(i.text)\n",
    "        if(len(brand)>99):\n",
    "            break\n",
    "         \n",
    "        #Storing product description in prod_desc list\n",
    "    for i in prdesc_tag:\n",
    "        prod_desc.append(i.text)\n",
    "        if(len(prod_desc)>99):\n",
    "            break\n",
    "        \n",
    "        #Storing price in price list\n",
    "    for i in price_tag:\n",
    "        price.append(i.text)\n",
    "        if(len(price)>99):\n",
    "            break\n",
    "        \n",
    "        #Clicking on next page and scraping other detail\n",
    "    nextpage=driver.find_element(By.XPATH, '//a[@class=\"_1LKTO3\"]')\n",
    "    nextpage.click()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d75f5483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Error\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "#loop will iterate three times to store elements from 3 pages\n",
    "for i in range(1,4):\n",
    "    try:\n",
    "        print_100sneakers()\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "    finally:\n",
    "        #Entering Nan or 0 if the element does not have a value\n",
    "        NaN=\"None\"\n",
    "        if(len(brand) < 100):\n",
    "            brand += (100-len(brand)) * [NaN]\n",
    "        elif(len(prod_desc) < 100):\n",
    "            prod_desc += (100-len(prod_desc)) * [NaN]\n",
    "        elif(len(price) < 100):\n",
    "            price += (100-len(price)) * [0]\n",
    "        else:\n",
    "            print(\" \")          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6a8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8be51a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data frame with first 100 elements in each\n",
    "df=pd.DataFrame({'Brand':brand[0:100],'Product Description':prod_desc[0:100],'Price':price[0:100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7162c10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZF - ALFIYA</td>\n",
       "      <td>Sneakers For Women</td>\n",
       "      <td>₹499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RapidBox</td>\n",
       "      <td>Mesh | Ultralightweight | Comfortable | Breath...</td>\n",
       "      <td>₹374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aadi</td>\n",
       "      <td>Puma Smash Wns v2 L Sneakers For Women</td>\n",
       "      <td>₹1,542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Super Stylish &amp; Trendy Combo Pack of 02 Pairs ...</td>\n",
       "      <td>₹544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹5,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sneakers For Women</td>\n",
       "      <td>₹1,747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Caracal Sneakers For Men</td>\n",
       "      <td>₹351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Unique &amp; Perfect Collection Combo Pack of 02 S...</td>\n",
       "      <td>₹650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Brand                                Product Description   Price\n",
       "0   ZF - ALFIYA                                 Sneakers For Women    ₹499\n",
       "1      RapidBox  Mesh | Ultralightweight | Comfortable | Breath...    ₹374\n",
       "2          aadi             Puma Smash Wns v2 L Sneakers For Women  ₹1,542\n",
       "3          PUMA  Super Stylish & Trendy Combo Pack of 02 Pairs ...    ₹544\n",
       "4        Chevit                                   Sneakers For Men    ₹399\n",
       "..          ...                                                ...     ...\n",
       "95          NaN                                   Sneakers For Men  ₹5,490\n",
       "96          NaN                                 Sneakers For Women  ₹1,747\n",
       "97          NaN                           Caracal Sneakers For Men    ₹351\n",
       "98          NaN                                   Sneakers For Men    ₹297\n",
       "99          NaN  Unique & Perfect Collection Combo Pack of 02 S...    ₹650\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e71935",
   "metadata": {},
   "source": [
    "# Q7: Go to the link - https://www.myntra.com/shoes Set second Price filter and Color filter to “Black”, as shown in the below image.\n",
    "\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image.\n",
    "\n",
    "Note: Applying the filter and scraping the data, everything should be done through code only and there\n",
    "should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8f3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Connect to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#opening myntra page on authomated chrome browser\n",
    "driver.get(\"https://www.myntra.com/shoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb8bb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on black color check box\n",
    "clbox = driver.find_element(By.XPATH,'//label[@class=\"common-customCheckbox\"]')\n",
    "clbox.click()\n",
    "\n",
    "#Clicking on second price range check box\n",
    "spr = driver.find_element(By.XPATH,\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/div\")\n",
    "spr.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a5d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = []\n",
    "prod_desc = []\n",
    "price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55d09dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_100blackshoes():\n",
    "    #finding three elements brand, product description and price\n",
    "    brand_tag=driver.find_elements(By.XPATH,'//h3[@class=\"product-brand\"]')\n",
    "    prdesc_tag=driver.find_elements(By.XPATH,'//h4[@class=\"product-product\"]')\n",
    "    price_tag=driver.find_elements(By.XPATH,'//div[@class=\"product-price\"]')\n",
    "        \n",
    "        #requesting the website to not change elements for the given seconds\n",
    "    time.sleep(1)\n",
    "        \n",
    "             \n",
    "         #Storing brand in brand list     \n",
    "    for i in brand_tag:\n",
    "        brand.append(i.text)\n",
    "        if(len(brand)>99):\n",
    "            break\n",
    "         \n",
    "        #Storing product description in prod_desc list\n",
    "    for i in prdesc_tag:\n",
    "        prod_desc.append(i.text)\n",
    "        if(len(prod_desc)>99):\n",
    "            break\n",
    "        \n",
    "        #Storing price in price list\n",
    "    for i in price_tag:\n",
    "        price.append(i.text)\n",
    "        if(len(price)>99):\n",
    "            break\n",
    "        \n",
    "        #Clicking on next page and scraping other detail\n",
    "    nextpage=driver.find_element(By.XPATH, '/html/body/div[2]/div/div[1]/main/div[3]/div[2]/div/div[2]/section/div[2]/ul/li[12]/a')\n",
    "    nextpage.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f68ccbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Error\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#loop will iterate three times to store elements from 3 pages\n",
    "for i in range(1,3):\n",
    "    try:\n",
    "        print_100blackshoes()\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "    finally:\n",
    "        #Entering Nan or 0 if the element does not have a value\n",
    "        NaN=\"None\"\n",
    "        if(len(brand) < 100):\n",
    "            brand += (100-len(brand)) * [NaN]\n",
    "        elif(len(prod_desc) < 100):\n",
    "            prod_desc += (100-len(prod_desc)) * [NaN]\n",
    "        elif(len(price) < 100):\n",
    "            price += (100-len(price)) * [0]\n",
    "        else:\n",
    "            print(\" \")        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da552564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data frame with first 100 elements in each\n",
    "df=pd.DataFrame({'Brand':brand,'Product Description':prod_desc,'Price':price})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a1cee4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skechers</td>\n",
       "      <td>Men Max Cushioning Running</td>\n",
       "      <td>Rs. 7199Rs. 8999(20% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men ZOOM WINFLO8 Running Shoes</td>\n",
       "      <td>Rs. 7465Rs. 8295(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men React Infinity 3 Running</td>\n",
       "      <td>Rs. 11895Rs. 13995(15% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men Colourblocked Sneakers</td>\n",
       "      <td>Rs. 10795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skechers</td>\n",
       "      <td>Men Max Cushioning Running</td>\n",
       "      <td>Rs. 7199Rs. 8999(20% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>fitflop</td>\n",
       "      <td>Women Leather Wedge Pumps</td>\n",
       "      <td>Rs. 8499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Black Woven Sneakers</td>\n",
       "      <td>Rs. 12999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Woven Design Sneakers</td>\n",
       "      <td>Rs. 10999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>J.FONTINI</td>\n",
       "      <td>Men Black Leather Loafers</td>\n",
       "      <td>Rs. 8490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Clarks</td>\n",
       "      <td></td>\n",
       "      <td>Rs. 7499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Brand             Product Description                        Price\n",
       "0     Skechers      Men Max Cushioning Running    Rs. 7199Rs. 8999(20% OFF)\n",
       "1         Nike  Men ZOOM WINFLO8 Running Shoes    Rs. 7465Rs. 8295(10% OFF)\n",
       "2         Nike    Men React Infinity 3 Running  Rs. 11895Rs. 13995(15% OFF)\n",
       "3         Nike      Men Colourblocked Sneakers                    Rs. 10795\n",
       "4     Skechers      Men Max Cushioning Running    Rs. 7199Rs. 8999(20% OFF)\n",
       "..         ...                             ...                          ...\n",
       "96     fitflop       Women Leather Wedge Pumps                     Rs. 8499\n",
       "97        ALDO        Men Black Woven Sneakers                    Rs. 12999\n",
       "98        ALDO       Men Woven Design Sneakers                    Rs. 10999\n",
       "99   J.FONTINI       Men Black Leather Loafers                     Rs. 8490\n",
       "100     Clarks                                                     Rs. 7499\n",
       "\n",
       "[101 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7496960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3b6f4",
   "metadata": {},
   "source": [
    "# Q8: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon.\n",
    "\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05976a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Connect to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#opening amazon page on authomated chrome browser\n",
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0c24f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserting Laptop in search box\n",
    "search = driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "search.send_keys('Laptop')\n",
    "\n",
    "#Clicking on search icon\n",
    "spr = driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\")\n",
    "spr.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77957275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on \"Intel Core i7” check box\n",
    "spr = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[4]/li[13]/span/a/div\")\n",
    "spr.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f03ca74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring 3 empty lists to store title, rating and price of laptops\n",
    "title = []\n",
    "rating = []\n",
    "price = []\n",
    "\n",
    "#finding three elements title, rating and price\n",
    "title_tag=driver.find_elements(By.XPATH,'//h2[@class=\"a-size-mini a-spacing-none a-color-base s-line-clamp-2\"]')\n",
    "\n",
    "rating_tag=driver.find_elements(By.XPATH,'//div[@class=\"a-row a-size-small\"]')\n",
    "\n",
    "price_tag=driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "        \n",
    "#requesting the website to not change elements for the given seconds\n",
    "time.sleep(1)\n",
    "        \n",
    "             \n",
    "#Storing title in title list     \n",
    "for i in title_tag:\n",
    "    title.append(i.text)\n",
    "         \n",
    "#Storing rating in rating list\n",
    "for i in rating_tag:\n",
    "    rating.append(i.text)\n",
    "        \n",
    "#Storing price in price list\n",
    "for i in price_tag:\n",
    "    price.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1dc31241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fujitsu UH-X 11th Gen Intel Core i7 13.3” FHD ...</td>\n",
       "      <td>68</td>\n",
       "      <td>84,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samsung Galaxy Book2 Intel 12th Gen core i7 39...</td>\n",
       "      <td>26</td>\n",
       "      <td>79,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Samsung Galaxy Book2 Intel 12th Gen core i7 39...</td>\n",
       "      <td>26</td>\n",
       "      <td>79,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...</td>\n",
       "      <td>26</td>\n",
       "      <td>57,985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASUS TUF Gaming F15 (2022), 15.6-inch (39.62 c...</td>\n",
       "      <td>24</td>\n",
       "      <td>1,09,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Acer Predator Helios 300 12th Gen Intel Core i...</td>\n",
       "      <td>4</td>\n",
       "      <td>1,54,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lenovo Ideapad Gaming 3 Intel Core i7 10th Gen...</td>\n",
       "      <td>23</td>\n",
       "      <td>62,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lenovo ThinkPad E14 Intel Core i7 11th Gen 14-...</td>\n",
       "      <td>3</td>\n",
       "      <td>94,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lenovo IdeaPad Slim 5 Intel Core i7 12th Gen 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>81,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ASUS TUF Gaming F15 (2022), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1,09,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title Rating     Price\n",
       "0  Fujitsu UH-X 11th Gen Intel Core i7 13.3” FHD ...     68    84,990\n",
       "1  Samsung Galaxy Book2 Intel 12th Gen core i7 39...     26    79,490\n",
       "2  Samsung Galaxy Book2 Intel 12th Gen core i7 39...     26    79,490\n",
       "3  ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...     26    57,985\n",
       "4  ASUS TUF Gaming F15 (2022), 15.6-inch (39.62 c...     24  1,09,990\n",
       "5  Acer Predator Helios 300 12th Gen Intel Core i...      4  1,54,990\n",
       "6  Lenovo Ideapad Gaming 3 Intel Core i7 10th Gen...     23    62,990\n",
       "7  Lenovo ThinkPad E14 Intel Core i7 11th Gen 14-...      3    94,990\n",
       "8  Lenovo IdeaPad Slim 5 Intel Core i7 12th Gen 1...      1    81,990\n",
       "9  ASUS TUF Gaming F15 (2022), 15.6\" (39.62 cms) ...      6  1,09,990"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Title':title[0:10],'Rating':rating[0:10],'Price':price[0:10]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e713d",
   "metadata": {},
   "source": [
    "# Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter “Data Scientist” and click on search button.\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter “Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c60941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Connect to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#opening amazon page on authomated chrome browser\n",
    "driver.get(\"https://www.ambitionbox.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da373b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on jobs\n",
    "jobs = driver.find_element(By.XPATH,'//a[@href=\"jobs\"]')\n",
    "jobs.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dfbdaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserting Data Scientist in search box\n",
    "search = driver.find_element(By.XPATH,'//input[@title=\"Enter Designation, Company or a Skill\"]')\n",
    "search.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da1eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on search button\n",
    "search = driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/button/span')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef8615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on location button\n",
    "search = driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[1]/i')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ccb796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[2]/input')\n",
    "search.send_keys('Noida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdaa65e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on Noida\n",
    "search = driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[3]/div[1]/div[1]/div/label')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c9e36fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Declaring 3 empty lists to store company, experience and salary \n",
    "com = []\n",
    "exp = []\n",
    "sal = []\n",
    "\n",
    "#finding three elements company, experience and salary\n",
    "com_tag=driver.find_elements(By.XPATH,'//div[@class=\"company-info\"]')\n",
    "\n",
    "exp_tag=driver.find_elements(By.CSS_SELECTOR, 'p.body-small-l')\n",
    "\n",
    "sal_tag=driver.find_elements(By.CSS_SELECTOR, 'p.body-small-l')\n",
    "#sal_tag=driver.find_elements(By.XPATH,'//*[contains(text(), \" LPA\")]')\n",
    "        \n",
    "#requesting the website to not change elements for the given seconds\n",
    "time.sleep(1)\n",
    "        \n",
    "             \n",
    "#Storing company name in com list     \n",
    "for i in com_tag:\n",
    "    com.append(i.text)\n",
    "         \n",
    "#Storing experience in exp list\n",
    "n=0\n",
    "for i in exp_tag:\n",
    "    if(n%4==0):\n",
    "        exp.append(i.text)\n",
    "    n+=1\n",
    "        \n",
    "#Storing salary in sal list\n",
    "for i in sal_tag:\n",
    "    sal.append(i.text)\n",
    "\n",
    "#storing every 4th element in new list for salary package\n",
    "newl = []\n",
    "for i in range(1,43,4):\n",
    "    newl.append(sal[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a37016b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBRE South Asia Pvt Ltd\\n · \\n4.3\\nbased on 2....</td>\n",
       "      <td>5-9 years</td>\n",
       "      <td>₹ 10-20 LPA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBRE South Asia Pvt Ltd\\n4.3\\n(2.3k Reviews)</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "      <td>₹ 10-20 LPA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GENPACT India Private Limited\\n4.0\\n(19.9k Rev...</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "      <td>Not Disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Genpact\\n4.0\\n(19.9k Reviews)</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ericsson India Global Services Pvt. Ltd.\\n4.3\\...</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GENPACT India Private Limited\\n4.0\\n(19.9k Rev...</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>Ghaziabad, Gurgaon/Gurugram, Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Optum Global Solutions (India) Private Limited...</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Optum Global Solutions (India) Private Limited...</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ericsson India Global Services Pvt. Ltd.\\n4.3\\...</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SOPRA STERIA INDIA LIMITED\\n4.2\\n(1.2k Reviews)</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>Chennai, Bengaluru/Bangalore, Noida</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Company     Experience  \\\n",
       "0  CBRE South Asia Pvt Ltd\\n · \\n4.3\\nbased on 2....      5-9 years   \n",
       "1       CBRE South Asia Pvt Ltd\\n4.3\\n(2.3k Reviews)        5-9 Yrs   \n",
       "2  GENPACT India Private Limited\\n4.0\\n(19.9k Rev...       7-12 Yrs   \n",
       "3                      Genpact\\n4.0\\n(19.9k Reviews)       6-10 Yrs   \n",
       "4  Ericsson India Global Services Pvt. Ltd.\\n4.3\\...  Not Disclosed   \n",
       "5  GENPACT India Private Limited\\n4.0\\n(19.9k Rev...  Not Disclosed   \n",
       "6  Optum Global Solutions (India) Private Limited...  Not Disclosed   \n",
       "7  Optum Global Solutions (India) Private Limited...  Not Disclosed   \n",
       "8  Ericsson India Global Services Pvt. Ltd.\\n4.3\\...  Not Disclosed   \n",
       "9    SOPRA STERIA INDIA LIMITED\\n4.2\\n(1.2k Reviews)  Not Disclosed   \n",
       "\n",
       "                                Salary  \n",
       "0                          ₹ 10-20 LPA  \n",
       "1                          ₹ 10-20 LPA  \n",
       "2                        Not Disclosed  \n",
       "3                                Noida  \n",
       "4                                Noida  \n",
       "5   Ghaziabad, Gurgaon/Gurugram, Noida  \n",
       "6                                Noida  \n",
       "7                                Noida  \n",
       "8                                Noida  \n",
       "9  Chennai, Bengaluru/Bangalore, Noida  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Company':com[0:10],'Experience':exp[0:10],'Salary':newl[0:10]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46fcdb",
   "metadata": {},
   "source": [
    "# Q10: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and then click on “Data Scientist”. \n",
    "\n",
    "You have to scrape the data ticked in the above image.\n",
    "\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average salary, minimum salary, maximum salary, experience required.\n",
    "\n",
    "5. Store the data in a dataframe. Note: All the steps required during scraping should be done through code only and not manually.You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6eb90fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Connect to the driver\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#opening amazon page on authomated chrome browser\n",
    "driver.get(\"https://www.ambitionbox.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3695e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on salaries\n",
    "sal = driver.find_element(By.XPATH,'/html/body/div[1]/nav[2]/div/ul/li[3]/a')\n",
    "sal.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4adf6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserting Data Scientist in search box\n",
    "search = driver.find_element(By.XPATH,'/html/body/section[1]/div/div/div/div[2]/form/span/input')\n",
    "search.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d918db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on search button\n",
    "sal = driver.find_element(By.XPATH,'/html/body/section[1]/div/div/div/div[2]/form/div[2]/input')\n",
    "sal.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3e717f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on page\n",
    "sal = driver.find_element(By.XPATH,'/html/body/section[1]/div/section/div/div/div/div/div/div/div[5]/div[2]/div/div/div[1]/div[1]/div[1]/div[1]/div/a')\n",
    "sal.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "776d38cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Declaring 6 empty lists to store company, experience, Total salary and min , max ,avg salary \n",
    "com = []\n",
    "exp = []\n",
    "comdtl = []\n",
    "minsal = []\n",
    "maxsal = []\n",
    "avgsal = []\n",
    "\n",
    "#finding 6 elements company, detail, experience and min , max ,avg salary\n",
    "\n",
    "com_tag=driver.find_elements(By.XPATH,'//*[@id=\"col-left\"]/div[5]/div[2]/div[2]/div[1]/div[1]/div/div/div[1]/a')\n",
    "\n",
    "exp_tag=driver.find_elements(By.XPATH, '//div[@class=\"sbold-list-header\"]')\n",
    "\n",
    "comdtl_tag=driver.find_elements(By.XPATH, '//span[@class=\"jp\"]')\n",
    "\n",
    "minsal_tag=driver.find_elements(By.CLASS_NAME,'value body-medium')\n",
    "\n",
    "maxsal_tag=driver.find_elements(By.XPATH, '//p[@class=\"averageCtc\"]')\n",
    "\n",
    "avgsal_tag=driver.find_elements(By.XPATH, '//div[@class=\"salary-values\"]')\n",
    "\n",
    "        \n",
    "#requesting the website to not change elements for the given seconds\n",
    "time.sleep(1)\n",
    "        \n",
    "             \n",
    "#Storing company in com list     \n",
    "for i in com_tag:\n",
    "    com.append(i.text)\n",
    "         \n",
    "#Storing experience in exp list\n",
    "for i in exp_tag:\n",
    "    exp.append(i.text)\n",
    "   \n",
    " #Storing company detail in comdtl list\n",
    "for i in comdtl_tag:\n",
    "    comdtl.append(i.text)\n",
    "\n",
    "           \n",
    "#Storing minimum salary in minsal list     \n",
    "for i in minsal_tag:\n",
    "    minsal.append(i.text)\n",
    "         \n",
    "#Storing maximum salary in maxsal list\n",
    "for i in maxsal_tag:\n",
    "    maxsal.append(i.text)\n",
    "            \n",
    "#Storing average salary in avgsal list\n",
    "for i in avgsal_tag:\n",
    "    avgsal.append(i.text)  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1cd42d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Company Detail</th>\n",
       "      <th>Min Salary</th>\n",
       "      <th>Max Salary</th>\n",
       "      <th>Avg Sal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Company, Experience, Company Detail, Min Salary, Max Salary, Avg Sal]\n",
       "Index: []"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Company':com[0:10],'Experience':exp[0:10],'Company Detail':comdtl[0:10],'Min Salary':minsal[0:10],'Max Salary':maxsal[0:10],'Avg Sal':avgsal[0:10]})\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
